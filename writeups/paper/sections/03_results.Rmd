```{r generate to-print model results}
MODERATORS <- c( "NULL", "mean_age","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence", "n_repetitions_video", "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "visual_stimuli_pair", "test_method","presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )
mod_print <- generate_moderator_df(MODERATORS, ma_data)
```

```{r label = "forest", fig.pos = "!t", fig.cap = "Each row in the forrest plot represents an individual effect size. The black circles correspond to individual condition using transitive sentences. The black triangles correspond to individual condition using intransitive sentences. The size of the circles and triangles indicate their weights, and the horizontal error bars show the 95% confidence intervals. The red diamond at the bottom is the meta-analytic effect size aggregated across all conditions included in the literature.",fig.width = 45, fig.height = 60}
generate_forest_plot(ma_data)
```

Figure\ \@ref(fig:forest) shows effect size estimates for all conditions in our sample. The weighted mean effect size was `r filter(mod_print, moderator == "NULL")$estimate_print`, which significantly differed from 0  (*Z* = `r filter(mod_print, moderator == "NULL")$z_print`; *p* `r filter(mod_print,  moderator == "NULL")$p_print`). There was evidence for considerable heterogenity in effect sizes across our sample (*Q* = `r filter(mod_print, moderator == "NULL")$Q_print`; *p* `r filter(mod_print, moderator == "NULL")$Qp_print`), meaning that there is unexplained variance in effect sizes aross studies.

## Evidential value of the syntactic bootstrapping literature

We first evaluated the evidential value of the literature by assessing the evidence for publication bias. Publication bias refers to the tendency to selectively publish positive findings. The intuition underlying these analyses is that a meta-analysis includes both studies that have statistically significant effect sizes for the target phenomenon and studies those do not. Critically, due to publication pressures, there are likely some studies that have been conducted that are not stastically significant but are not part of our meta-analysis. The absence of these studies from the meta-analysis leads to a meta-analytic estimate that over-estimates the true effect size, and threatens the evidential value of the literature. We present two analyses of publication bias: a classic funnel plot analysis, and a sensitivity analysis that assumes a more plausible model of publication bias. 

```{r label = "funnel", fig.pos = "t!", fig.height = 3.75, fig.width = 4, fig.cap = "Funnel plot showing the standard error of each effect size estimate in our meta-analysis as a function of the magnitude of that effect size. The gray and red vertical dashed lines correspond to an effect size of zero and the meta-analytic effect size estimate, respectively. The grey funnel represents a 95% confidence interval around meta-analytic estimate. In the absence of publication bias, effect size estimates should by symmetrically distributed around the red line."}
generate_funnel_plot(ma_data)

reg_result <- rma.mv(d_calc ~ sqrt(d_var_calc),  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=ma_data) # access zval and pval from here
reg_z <- round(reg_result$zval, digits = 2)
p_val <- reg_result$pval[2]
reg_p <- case_when(
 p_val < 0.0001 ~ "< .0001", 
 TRUE ~ as.character(p_val))
#reg_result
```

<!-- [DONE?]the above method of calculating egger's test does not take into account the clustered structure of our data, as we do elsewhere. There's no way to add this structure using the regtest function; you'll need to do the manual regression method that I added to the markdown on publication bias to get the correct values. [DONE] The red line on the funnel itself should also reflect the model estimate from the correct ma model. -->

Figure\ \@ref(fig:funnel) presents the funnel plot for the effect sizes in our sample. A funnel plot shows estimates of effect size variance (plotted with large values lower on the axis) as a function of the magnitude of the effect size [@egger1997bias]. Under a model of publication bias in which researchers decide whether or not to publish a study based on the magnitude of its effect size (larger effect sizes being more likely), effect size estimates should fall symmetrically around the grand effect size estimate. Evidence of asymmetry around the grand mean, particularly more large, positive effect sizes, would suggest that the literature reflects a biased sample of studies. A formal test of asymmetry in our sample revealed evidence for asymmetry (Egger's test: *Z* = `r reg_z`; *p* `r reg_p`).

The funnel plot analysis provides some evidence for publication bias, but the interpretation of this analysis is limited by the fact that it assumes a relatively implausible model of how researchers decide which studies to make public: the criteria for publishing a study in a journal is typically not the *size* of the effect, as assumed by the funnel plot analysis, but rather whether or not the p-value of the hypothesis test for that effect is below some threshold (usually .05). We therefore conducted a second analysis of publication bias, called a sensitivity analysis [@mathur2020sensitivity], which assumes that the decision to publish results is determined by the size of the p-value, rather than the magnitude of the effect size.

<!-- I took out the bit about hetergenity since there are ways to address hetergenity using funnel plots - i.e. just include them in the model-->

The goal of the sensitivity analysis is to determine how sensitive the meta-analytic effect size is to "missing" non-significant studies. Critically, because the degree of publication is not known (i.e. the degree to which significant results are more likely to be published, relative to insignificant results), the sensitivity analysis assumes a worst-case publication bias scenario and estimates the meta-analytic effect size under this scenario. The worst-case scenario assumed by the model is that significant studies are infinitely more likely to be published than non-significant studies ^[Technically, the model assumes studies with effect sizes that are statistically significant (*p* < .05) *and* greater than zero are infintely more likey to be published. See Mathur and VanderWeele (2020) for additional details.]. A meta-analytic effect size under this scenario can be estimated by analyzing only those studies with significant effect size estimates.

```{r}
ma_data_with_affirm <- ma_data %>%
  mutate(pvalue =  2 * (1 - pnorm( abs(d_calc / sqrt(d_var_calc)))),
         affirm =  (d_calc > 0) & (pvalue < 0.05))

affirm_model<- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/x_1, data=
          ma_data_with_affirm %>% filter(affirm == FALSE)) 

worst_case_estimate_print <- paste0(as.numeric(round(affirm_model$beta, 2)),
                                    " [",
                                    as.numeric(round(affirm_model$ci.lb, 2)),
                                    ", ",                                                                                                        as.numeric(round(affirm_model$ci.ub, 2)),
                                    "]"
)
```

Conducting this sensitivity analysis on our data reveals that no amount of publication bias could attentuate the point estimate of the effect size to 0. Nevertheless, the worse-case scenario appreciably attenuates the meta-analytic effect size, and the attenuated effect size estimate includes 0 in its 95% confidence interval (`r worst_case_estimate_print`; see SI for additional details). 

In sum, across two types of analyses, we find some evidence for publication bias in the synactic bootstrapping literature, but even under worst-case scenarios publication bias was not enough to fully attenuate the meta-analytic point estimate to 0. Further, some of the publication bias observed in the funnel plot analysis may be due to heterogeneity in the data. In the following sections, we analyze theoretical and methodological moderators that could be contributing to this heterogeneity, though we emphasize that the likely presence of publication bias implies that these moderators should be interpreted with caution. 

## Relating the syntactic bootstrapping effect to other word learning strategies

```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
MAPPING_STUDIES <- c("Mutual exclusivity", 
                     "Sound symbolism",
                     "Cross-situational word learning",
                     "Gaze following")


all_models_res_younger <- map_df(MAPPING_STUDIES, get_model_results_younger_than, 48)
SB_res_younger <- fit_model(filter(ma_data, mean_age < 30.44 * 48), "Syntactic Bootstrapping")
all_models_res_younger <- bind_rows(all_models_res_younger, SB_res_younger) 
tidy_all_models_res_younger <-  tidy_metalab_df(all_models_res_younger)
tidy_all_age_summary <- summarize_metalab_age_younger_than(ma_data, 48)
tidy_models_with_age_younger <- left_join(tidy_all_models_res_younger, 
                                  tidy_all_age_summary, 
                                  by = "dataset_name") 
ML_print <- get_metalab_es_print(tidy_all_models_res_younger)

```

```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide', label = "metalab", fig.pos="H", fig.cap= "Each dot represents a meta-analytic effect size. The size of the dot represents the number of individual effect sizes included in that particular meta-analysis. The x-axis corresponds to the magnitude of the esimated effect size. The y-axis represents the mean age of the infants included in each meta-analysis."}
generate_metalab_plot(tidy_models_with_age_younger)
```


In order to learn words, children must figure out the mapping relationships between the words and their references. Previous research has proposed numerous biases and learning strategies by which children can solve the local mapping problem. We collected an opportunity sample of meta-analyses on the related learning strategies through publicly available datasets on Metalab and can be accessed with the metalabr package (CITATION?). For more direct comparison across the learning phenomena, we focused on children younger than 48-month-olds. We included Mutual exclusivity (*d* = `r filter(ML_print, dataset_name == "Mutual exclusivity")$estimate_print_full`; Mean age: `r filter(tidy_all_age_summary, dataset == "Mutual exclusivity")$mean_age` Days), Cross-situational word learning (*d* = `r filter(ML_print, dataset_name == "Cross-situational word learning")$estimate_print_full`; Mean age: `r filter(tidy_all_age_summary, dataset == "Cross-situational word learning")$mean_age` Days ), Gaze following (*d* = `r filter(ML_print, dataset_name == "Gaze following")$estimate_print_full`; Mean age: `r filter(tidy_all_age_summary, dataset == "Gaze following")$mean_age` Days ) and Sound symbolism(*d* = `r filter(ML_print, dataset_name == "Sound symbolism")$estimate_print_full`; Mean age: `r filter(tidy_all_age_summary, dataset == "Sound symbolism")$mean_age` Days). Figure \@ref(fig:metalab) shows syntactic bootstrapping's effect size is relatively small compared to other learning strategies. The discrepancy might caused by the other word learning strategies were studied exclusively with nouns. 


## Theoretical Moderators

We next asked whether the overall effect size estimate was moderated by our theoretical moderators of interest: development-related moderators (vocabulary and age), and sentence structure moderators (predicate and noun phrase types).

```{r label = "megaPlot", fig.width = 10, fig.height = 5,fig.pos = "T!", fig.cap = "In these figures we presented moderator estimates from two types of model: the red dots represent moderator estimates from single-predictor model, and the gray dots connected with each other represent moderator estimates from the full model including all the moderators from the group. For categorical variables, the dots represent the level that appears first in the parentheses. We found that each theoretical moderator and methodological moderator has similar estimates in the single-predictor models and the full models.  (a) Predicate type is a significant predictor for effect size. In particular, transitive sentence has a positive effect on the effect size relative to intransitive sentence. Note that in this plot we did not include median productive vocabulary size. This is because only a subset of studies reported the information. (b) Testing structure and sentence reptitions marginally predict.."}

theoretical_mod <- rma.mv(d_calc ~ sentence_structure + agent_argument_type + mean_age, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data) 

theoretical_mod_print <- generate_mega_model_df(theoretical_mod)

methodological_mod <- rma.mv(d_calc ~ character_identification + practice_phase + presentation_type + test_mass_or_distributed + n_repetitions_sentence,
                             V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data)
methodological_mod_print <- generate_mega_model_df(methodological_mod)


p_theoretical <- generate_predictor_plot(mod_print, theoretical_mod_print, "theoretical") +
  theme(legend.position = "none") + 
  ggtitle("a. Theoretical Moderators")

p_model <- generate_predictor_plot(mod_print, methodological_mod_print, "methodological") + 
  ggtitle("b. Methodological Moderators")


p_model_pane <- p_theoretical + p_model

p_model_pane  

```


### Development

```{r}
age_vocab_cor <- cor.test(ma_data$mean_age,ma_data$productive_vocab_median)
cor_p <- ifelse(round(age_vocab_cor$p.value, digits = 2)<0.0001, "< 0.0001", as.character(round(age_vocab_cor$p.value, digits = 2)))
```

<!-- Things to change in the plot: [done](1) add caption, [done](2) change x axis to be months [done](3) I know we've gone back and forth about this, but let's just include the simple lm model fit. I think having both is confusing. -->
```{r, label = "development", fig.cap = "Effect size (Cohen's d) as a function of age in months. The dash line corresponds to effect size equals zero, the solid red regression line is a simple linear model fit, and the shaded area corresponds to the standard error of the fitted model. Each dot represents one calcaulated effect size, and the size of the dot corresponds to the number of infants included in that condition."}

ma_data %>%
  ggplot() +
  geom_point(aes(x = mean_age, y = d_calc, size = n_1)) +
  geom_smooth(method = "lm",aes(x = mean_age, y = d_calc)) +
  scale_x_continuous(labels = function(x) round(x/30.44), 
                     breaks = seq(12,48*30.44, 6*30.44)) +
  geom_hline(yintercept = 0, linetype = "dashed")+
  theme_classic() + 
  geom_smooth(method = "lm",
              aes(x = mean_age, y = d_calc,color = "Cohen's d"),
              ) +
  #geom_smooth(method = "lm",
  #            aes(x = developmental_predictor, y = pred,
  #                color = "Model's Predicted Values")) +
  xlab("Mean Age (Months)") + 
  ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))   +
  guides(colour = FALSE, size = FALSE)
```

Does the syntactic bootstrapping effect get stronger across development? We examined two measures of developmental change: age and vocabulary size. These two measures were strongly correlated with each other (*r*(`r age_vocab_cor$parameter`) = `r round(age_vocab_cor$estimate, digits = 2)`, *p*: `r cor_p`). Age ($\beta$ = `r filter(mod_print, moderator == "mean_age")$mod_estimate_print_full`, *SE* `r filter(mod_print, moderator == "mean_age")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "mean_age")$mod_z_print`, *p* `r filter(mod_print, moderator == "mean_age")$mod_p_print`; Fig.\@ref(fig:megaPlot); Fig.\@ref(fig:development)a) is not a significant moderator of the effect. However, with the subset of the studies with vocabulary information available, vocabulary size is a marginally significant moderator ($\beta$ = `r filter(mod_print, moderator == "productive_vocab_median")$mod_estimate_print_full`, *SE* `r filter(mod_print, moderator == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_z_print`, *p* `r filter(mod_print, moderator == "productive_vocab_median")$mod_p_print`).

### Sentence structure 
```{r summarize_sentence_structure}
ss_summary <- ma_data %>% 
  group_by(sentence_structure) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )
```
<!-- throughout, let's change what we have been calling "sentence structure" to be "predicate type" and use sentence structure to refer to both predicate type and noun phrase type --> 

We next asked how properties of the sentence struture that children heard influenced the strength of the syntactic bootstrapping effect. Predicate type (transitive vs. intransitive) was a significant moderator, ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "sentence_structure")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "sentence_structure")$mod_z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$mod_p_print`), with the effect being larger for transitive conditions (*M* = `r filter(ss_summary, sentence_structure == "transitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "transitive")$sd`) relative to intransitive conditions (*M* = `r filter(ss_summary, sentence_structure == "intransitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "intransitive")$sd`). In contrast, there was no effet of agent argument type (pronoun vs. noun; $\beta$ = `r filter(mod_print, moderator == "agent_argument_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "agent_argument_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "agent_argument_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "agent_argument_type")$mod_p_print`). 

To compare the effects of all theoretical moderators, we fit an additive model with all theoretical variables as fixed effects. We excluded vocabulary size from the additive model since it was highly correlated with age, and only available for a subset of conditions (*N* = `r  num_vocabulary_available`). Figure \@ref(fig:megaPlot) shows estimates for each of the single-predictor models along with the additive linear model. The additive model revelaed estimates that were highly comproable to the single-predictor model.  

In summary, we found that predicate type is a significant predictor of the effect size. Conditions with transitive sentences were associated with larger effect sizes than those tested with intransitive sentences. No other theoretical variable significantly moderated the effect.


## Methodological Moderators 

One limiting factor in interpetting the moderating  role of theoretical vairables on the syntactic bootstrapping effect is that there was variability across studies in the exact method used in testing children. It is possible that this methodological variability conceals true underlying moderating influences. For example, if researchers adapt their method to the age of the children they are targeting, this might conceal developmental change in the strength of the effect [@bergmann2018promoting]. 

We next asked whether the five different methodological variables (practice phase, sentence repetitions, character identifcation phase, synchronicity, testing procedure structure) moderated the syntactic bootstrapping effect. None of these methodological variables were significant moderators of the effect in a single predictor model (Fig. \@ref(fig:megaPlot); see SI for exact estimates). In an additive linear model with all five methodological predictors, there was a marginally significant effect of testing procedure structure ($\beta$ = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_p_print`), with distributed testing designs tending to have larger effect sizes than mass designs. This finding suggests that children tested in a procedure with only one train-test pair performed better than those tested in procedure with multiple train-test pairs. Finally, we asked how these methodological moderators related to our theoretical moderators of interest. Controlling for methodological variables did not qualitiatively change the role of any of the theoretical moderators (see SI).  Taken together, these analyses suggest that methodological variables do not play a large influencing role on the size of the syntactic bootstrapping effect. 

<!-- rather than list all the (null, not-very-interesting) estimates in the main text, lets add tables to the SI with the exact values from the theoretical and methodological plots --> 
