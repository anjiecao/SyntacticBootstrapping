```{r generate to-print model results}
MODERATORS <- c( "NULL", "mean_age_months","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence", "n_repetitions_video", "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "visual_stimuli_pair", "test_method","presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )
mod_print <- generate_moderator_df(MODERATORS, ma_data)
```

```{r label = "forest", fig.pos = "!t", fig.cap = "Each row in the forrest plot represents an individual effect size. The black circles correspond to individual condition using transitive sentences. The black triangles correspond to individual condition using intransitive sentences. The size of the circles and triangles indicate their weights, and the horizontal error bars show the 95% confidence intervals. The red diamond at the bottom is the meta-analytic effect size aggregated across all conditions included in the literature.",fig.width = 45, fig.height = 60}
#generate_forest_plot(ma_data)
```

Figure\ \@ref(fig:forest) shows effect size estimates for all conditions in our sample. The weighted mean effect size was `r filter(mod_print, moderator == "NULL")$estimate_print`, which significantly differed from 0  (*Z* = `r filter(mod_print, moderator == "NULL")$z_print`; *p* `r filter(mod_print,  moderator == "NULL")$p_print`). There was evidence for considerable heterogeneity in effect sizes across our sample (*Q* = `r filter(mod_print, moderator == "NULL")$Q_print`; *p* `r filter(mod_print, moderator == "NULL")$Qp_print`), meaning that there is unexplained variance in effect sizes across studies.

## Evidential value of the syntactic bootstrapping literature

We first evaluated the evidential value of the literature by assessing the evidence for publication bias.  The intuition underlying these analyses is that, due to random variation, a literature should be expected to contain studies both with and without statistically significant effect sizes for the target phenomenon. Critically, however, publication pressures may lead researchers to be more likely to publish findings with statistically significant results, resulting in a biased literature. The absence of these studies from the meta-analysis yields a meta-analytic estimate that over-estimates the true effect size, and threatens the evidential value of the literature. We present two analyses that assess publication bias in the syntactic bootstrapping literature: a classic funnel plot analysis, and a sensitivity analysis that assumes a more plausible model of the publication process. 

```{r label = "funnel", fig.pos = "t!", fig.height = 3.75, fig.width = 4, fig.cap = "Funnel plot showing the standard error of each effect size estimate in our meta-analysis as a function of the magnitude of that effect size. The gray and red vertical dashed lines correspond to an effect size of zero and the meta-analytic effect size estimate, respectively. The grey funnel represents a 95% confidence interval around meta-analytic estimate. In the absence of publication bias, effect size estimates should be symmetrically distributed around the red line."}
generate_funnel_plot(ma_data)

reg_result <- rma.mv(d_calc ~ sqrt(d_var_calc),  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=ma_data) # access zval and pval from here
reg_z <- round(reg_result$zval, digits = 2)
p_val <- reg_result$pval[2]
reg_p <- case_when(
 p_val < 0.0001 ~ "< .0001", 
 TRUE ~ as.character(p_val))
#reg_result
```

<!-- [DONE?]the above method of calculating egger's test does not take into account the clustered structure of our data, as we do elsewhere. There's no way to add this structure using the regtest function; you'll need to do the manual regression method that I added to the markdown on publication bias to get the correct values. [DONE] The red line on the funnel itself should also reflect the model estimate from the correct ma model. -->

Figure\ \@ref(fig:funnel) presents the funnel plot for the effect sizes in our sample. A funnel plot shows estimates of effect size variance (plotted with large values lower on the axis) as a function of the magnitude of the effect size [@egger1997bias]. Under a model of publication bias in which researchers decide whether or not to publish a study based on the magnitude of its effect size (larger effect sizes being more likely), effect size estimates should fall symmetrically around the grand effect size estimate. Evidence of asymmetry around the grand mean, particularly more large, positive effect sizes, would suggest that the literature reflects a biased sample of studies. A formal test of asymmetry in our sample revealed evidence for asymmetry (Egger's test: *Z* = `r reg_z`; *p* `r reg_p`).

The funnel plot analysis provides some evidence for publication bias, but the interpretation of this analysis is limited by the fact that it assumes a relatively implausible model of how researchers decide which studies to make public: the criteria for publishing a study in a journal is typically not the *size* of the effect, as assumed by the funnel plot analysis, but rather whether or not the p-value of the hypothesis test for that effect is below some threshold (usually .05). We therefore conducted a second analysis of publication bias, called a sensitivity analysis [@mathur2020sensitivity], which assumes that the decision to publish results is determined by the size of the p-value, rather than the magnitude of the effect size.

<!-- I took out the bit about heterogeneity since there are ways to address heterogeneity using funnel plots - i.e. just include them in the model-->

The goal of the sensitivity analysis is to determine how sensitive the meta-analytic effect size is to "missing" non-significant studies. Critically, because the degree of publication is not known (i.e. the degree to which significant results are more likely to be published, relative to insignificant results), the sensitivity analysis assumes a worst-case publication bias scenario and estimates the meta-analytic effect size under this scenario. The worst-case scenario assumed by the model is that significant studies are infinitely more likely to be published than non-significant studies ^[Technically, the model assumes studies with effect sizes that are statistically significant (*p* < .05) *and* greater than zero are infinitely more likely to be published. See Mathur and VanderWeele (2020) for additional details.]. A meta-analytic effect size under this scenario can be estimated by analyzing only those studies with non-significant effect size estimates.

```{r}
ma_data_with_affirm <- ma_data %>%
  mutate(pvalue =  2 * (1 - pnorm( abs(d_calc / sqrt(d_var_calc)))),
         affirm =  (d_calc > 0) & (pvalue < 0.05))

affirm_model<- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/x_1, data=
          ma_data_with_affirm %>% filter(affirm == FALSE)) 

worst_case_estimate_print <- paste0(as.numeric(round(affirm_model$beta, 2)),
                                    " [",
                                    as.numeric(round(affirm_model$ci.lb, 2)),
                                    ", ",                                                                                                        as.numeric(round(affirm_model$ci.ub, 2)),
                                    "]"
)
```

Conducting this sensitivity analysis on our data reveals that no amount of publication bias could attenuate the point estimate of the effect size to 0. Nevertheless, the worst-case scenario appreciably attenuates the meta-analytic effect size, and the attenuated effect size estimate includes 0 in its 95% confidence interval (`r worst_case_estimate_print`; see SI, Sec.2 for additional details). 

In sum, across two types of analyses, we find some evidence for publication bias in the syntactic bootstrapping literature, but even under worst-case scenarios publication bias was not enough to fully attenuate the meta-analytic point estimate to 0. Further, some of the publication bias observed in the funnel plot analysis may be due to heterogeneity in the data. In the following sections, we analyze theoretical and methodological moderators that may contribute to this heterogeneity, though we emphasize that the likely presence of publication bias implies that these moderators should be interpreted with caution. 

## Relating the syntactic bootstrapping effect to other word learning strategies

```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
MAPPING_STUDIES <- c("Mutual exclusivity", 
                     "Sound symbolism",
                     "Cross-situational word learning",
                     "Gaze following")

metalab_data <- download_metalab_data()

all_models_res_younger <- map_df(MAPPING_STUDIES, 
                                 get_model_results_younger_than, 
                                 48, 
                                 metalab_data)
SB_res_younger <- fit_model(filter(ma_data, mean_age < 30.44 * 48), "Syntactic Bootstrapping")
all_models_res_younger <- bind_rows(all_models_res_younger, SB_res_younger) 

tidy_all_models_res_younger <-  tidy_metalab_df(all_models_res_younger)
tidy_all_age_summary <- summarize_metalab_age_younger_than(ma_data, 
                                                           48, 
                                                           metalab_data)
tidy_models_with_age_younger <- left_join(tidy_all_models_res_younger, 
                                  tidy_all_age_summary, 
                                  by = "dataset_name") 
ML_print <- get_metalab_es_print(tidy_all_models_res_younger)

```

```{r echo=FALSE, results='hide', label = "metalab", fig.height = 3.75, fig.width = 6, fig.cap= "Meta-analytic effect sizes of five word learning phenomena, including syntactic bootstrapping (red). Point size corresponds to  the number of individual conditions included in each meta-analysis. The x-axis shows the magnitude of the meta-analytic effect size estimate; the y-axis shows the mean age in months of children in each meta-analysis."}
#generate_metalab_plot(tidy_models_with_age_younger)


label_position <- tibble(
  "label" = c("Mutual Exclusivity", 
              "Sound Symbolism",
              "Cross-Situational Word Learning",
              "Gaze Following", 
               "Syntactic Bootstrapping"
              ), 
  color = c(
    "black", 
    "black", 
    "black", 
    "black", 
    "red"
  ),
  label_x = c(tidy_models_with_age_younger$mean_age_in_month[1:4]+1.8,
            tidy_models_with_age_younger$mean_age_in_month[5]-1.8), 
  label_y = tidy_models_with_age_younger$estimate
)

tidy_models_with_age_younger %>% 
    mutate(
      text_color = case_when(
        dataset_name == "Syntactic Bootstrapping" ~ "red", 
        TRUE ~  "black"
      ), 
      dataset_name = case_when(
        dataset_name == "Cross-situational word learning" ~ "Cross-Situational Word Learning",
      #  dataset_name == "Syntactic Bootstrapping" ~ "Syntactic \n Bootstrapping",
        dataset_name == "Mutual exclusivity" ~ "Mutual Exclusivity",
        dataset_name == "Gaze following" ~ "Gaze Following", 
        dataset_name == "Sound symbolism" ~ "Sound Symbolism", 
        TRUE ~ dataset_name
      )
    ) %>% 
    ggplot(aes(x = mean_age_in_month,
               y = estimate,
               color = text_color
    )) + 
   # guides(size = FALSE) +
    xlim(0,36) +
    coord_flip() +
    geom_point(aes(size = num_study)) +
    scale_color_manual(values = c("black", "red")) + 
    guides(color = FALSE) +
    geom_text(data = label_position, 
              aes(label = label, 
                  x = label_x, 
                  y = label_y, 
                  color = color, 
                 ), size = 2)+
    #ggrepel::geom_text_repel(aes(label = dataset_name), size = 2.5, point.padding = .35, min.segment.length = 1) +
    #geom_text(aes(label = dataset_name), hjust = 0.5, vjust = -.5, size = 3) +
    geom_linerange(aes(ymin = estimate.cil, 
                       ymax = estimate.cih),size = 0.5) +
    geom_hline(yintercept = 0, color = "black", linetype="dashed")+
    scale_size_continuous(name = expression(paste(italic(N), " conditions"))) +
    ggtitle("Effect size estimates of word learning phenomena") +
    #theme(
    # axis.line = element_line(size = 1.2),
    # axis.ticks = element_line(size = 1)) +
    xlab("Mean Age (months)") + 
    ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))



```


How does the strength of the syntactic bootstrapping effect compare to that of other word learning strategies? To answer this question, we compared the meta-analytic syntactic bootstrapping effect size to effect sizes for other word learning strategies estimated from a meta-analysis of each literature. We considered an opportunity sample of word learning strategies, based on those strategies with available meta-analytic data. In particular, we selected all word learning strategies available in a database of language acquisition meta-analyses, called Metalab [@bergmann2018promoting]. We included a word learning strategy in our analysis if it could be considered to facilitate an inference about which word form was associated with a particular object.  This allowed for the comparison of the syntactic bootstrapping effect to four additional word learning strategies: (i) mutual exclusivity  [@lewis2020role; @markman1988children; @clark1987principle], assuming a novel word refers to a novel object, (ii) cross-situational word learning [@yu2007rapid], tracking word-object co-occurrences across situations, (iii) gaze following [@frank2016performance; @scaife1975capacity], following the eye gaze of a speaker to the intended referent, and (iv) sound symbolism [@fort2018symbouki], exploiting sound-meaning regularities in the lexicon. While these four strategies are not exhaustive of the strategies that have been proposed, they are representative of the major theoretical perspectives in the word learning literature, including constraints and biases [@markman1990constraints], statistical learning [@romberg2010statistical], and communicative inferences [@tomasello2010origins]. For each of these four comparison strategies, we calculated the meta-analytic effect size using the same model specification as for syntactic bootstrapping, restricting the sample to studies with a mean age of children younger than  48-month-olds.

Figure \@ref(fig:metalab) shows the meta-analytic effect size for syntactic bootstrapping and each of the other four word learning strategies.  The syntactic bootstrapping effect size (`r filter(mod_print, moderator == "NULL")$estimate_print`) was comparable in size to that of sound symbolism (*d* = `r filter(ML_print, dataset_name == "Sound symbolism")$estimate_print_full`; Mean age: `r round(filter(tidy_all_age_summary, dataset == "Sound symbolism")$mean_age_in_month, 1)` Months) and cross-situational learning (*d* = `r filter(ML_print, dataset_name == "Cross-situational word learning")$estimate_print_full`; Mean age: `r round(filter(tidy_all_age_summary, dataset == "Cross-situational word learning")$mean_age_in_month, 1)` Months ), and less than a quarter of the size of both mutual exclusivity (*d* = `r filter(ML_print, dataset_name == "Mutual exclusivity")$estimate_print_full`; Mean age: `r round( filter(tidy_all_age_summary, dataset == "Mutual exclusivity")$mean_age_in_month, 1)` Months) and gaze following (*d* = `r filter(ML_print, dataset_name == "Gaze following")$estimate_print_full`; Mean age: `r round(filter(tidy_all_age_summary, dataset == "Gaze following")$mean_age_in_month, 1)` Months). Importantly, the small effect size of syntactic bootstrapping relative to mutual exclusivity and gaze following cannot  be due alone to differences in the ages of the samples in these different meta-analyses, since participants in the syntactic bootstrapping meta-analysis were older on average than those in the gaze following meta-analysis, and roughly the same age as those in the mutual exclusivity meta-analysis. 

<!--The discrepancy might be caused by the other word learning strategies were studied exclusively with nouns. THIS IS AN IMPORTANT POINT, but let's move this to the GD -->


## Theoretical Moderators

We next asked whether the overall effect size estimate was moderated by our theoretical moderators of interest: development-related moderators (vocabulary and age), and sentence structure moderators (predicate and noun phrase types).


### Development

```{r}
age_vocab_cor <- cor.test(ma_data$mean_age_months,ma_data$productive_vocab_median)
cor_p <- ifelse(round(age_vocab_cor$p.value, digits = 2)<0.0001, "<.0001", as.character(round(age_vocab_cor$p.value, digits = 2)))
```

```{r, label = "development", fig.height = 4.4, fig.width = 4.5, fig.cap = "Syntactic bootstrapping effect size (Cohen's d) as a function of age in months. Each point corresponds to one effect size (condition), and point size corresponds to the number of children  in that condition. The blue line shows a linear model fit and the corresponding standard error. The slope of the model fit does not significantly differ from zero, suggesting no appreciable developmental change in the size of the syntactic bootstrapping effect as a function of age.  The dashed line indicates an effect size of zero. "}

ma_data %>%
  ggplot() +
  geom_point(aes(x = mean_age, y = d_calc, size = n_1),  alpha = .7) +
  geom_smooth(method = "lm",aes(x = mean_age, y = d_calc)) +
  scale_x_continuous(labels = function(x) round(x/30.44), 
                     breaks = seq(12,48*30.44, 6*30.44)) +
  geom_hline(yintercept = 0, linetype = "dashed")+
  geom_smooth(method = "lm",
              aes(x = mean_age, y = d_calc),
              ) +
  #geom_smooth(method = "lm",
  #            aes(x = developmental_predictor, y = pred,
  #                color = "Model's Predicted Values")) +
  ggtitle("Syntactic bootstrapping effect across development") +
  xlab("Mean Age (months)") + 
  ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))   +
  guides(colour = FALSE, size = FALSE)
```

```{r label = "megaPlot", fig.width = 10, fig.height = 5, fig.cap = "Meta-analytic models parameter estimates for (a) theoretical and (b) methodological moderators. Blue points show model estimates from single-predictor model; grey points show model estimates from additive linear model with all moderators included. Ranges correspond to 95% confidence intervals. Levels for categorical variables are given in parentheses, with the first level indicating the base level in the model."}


theoretical_mod <- rma.mv(d_calc ~ sentence_structure + agent_argument_type + mean_age_months, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data) 

theoretical_mod_print <- generate_mega_model_df(theoretical_mod)

methodological_mod <- rma.mv(d_calc ~ character_identification + practice_phase + presentation_type + test_mass_or_distributed + n_repetitions_sentence,
                             V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data)
methodological_mod_print <- generate_mega_model_df(methodological_mod)


p_theoretical <- generate_predictor_plot(mod_print, theoretical_mod_print, "theoretical") +
  theme(legend.position = "none") + 
  ggtitle("Theoretical Moderators") +
  plot_annotation(tag_levels = 'a')

p_model <- generate_predictor_plot(mod_print, methodological_mod_print, "methodological") + 
  ggtitle("Methodological Moderators") +
  plot_annotation(tag_levels = 'a')

p_model_pane <- p_theoretical + p_model

p_model_pane  

```

How does the strength of the syntactic bootstrapping effect change across development? We examined two measures of developmental change: age (months) and vocabulary size. These two measures were strongly correlated with each other (*r*(`r age_vocab_cor$parameter`) = `r round(age_vocab_cor$estimate, digits = 2)`, *p* `r cor_p`). There was no effect of age ($\beta$ = `r filter(mod_print, moderator == "mean_age_months")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "mean_age_months")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "mean_age_months")$mod_z_print`, *p* `r filter(mod_print, moderator == "mean_age_months")$mod_p_print`;  Fig. \@ref(fig:development)) or vocabulary ($\beta$ = `r filter(mod_print, moderator == "productive_vocab_median")$mod_estimate_print_full`, *SE* `r filter(mod_print, moderator == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_z_print`, *p* `r filter(mod_print, moderator == "productive_vocab_median")$mod_p_print`) on effect size (Fig. \@ref(fig:megaPlot)a).

### Sentence structure 
```{r summarize_sentence_structure}
ss_summary <- ma_data %>% 
  group_by(sentence_structure) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )
```

We next asked how properties of the sentence structure that children heard influenced the strength of the syntactic bootstrapping effect. Predicate type (transitive vs. intransitive) was a significant moderator, ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "sentence_structure")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "sentence_structure")$mod_z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$mod_p_print`). Notably, the intercept for predicate model was not significantly different from zero ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$estimate_print_full`, *z* = `r filter(mod_print, moderator == "sentence_structure")$z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$p_print`), which suggests that the effect is only present in transitive conditions (*M* = `r filter(ss_summary, sentence_structure == "transitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "transitive")$sd`) but not in intransitive conditions (*M* = `r filter(ss_summary, sentence_structure == "intransitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "intransitive")$sd`). In contrast, there was no effect of agent argument type (pronoun vs. noun; $\beta$ = `r filter(mod_print, moderator == "agent_argument_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "agent_argument_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "agent_argument_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "agent_argument_type")$mod_p_print`). 

To compare the effects of all theoretical moderators, we fit an additive model with all theoretical variables as fixed effects. We excluded vocabulary size from the additive model since it was highly correlated with age, and only available for a subset of conditions (*N* = `r  num_vocabulary_available`). Figure \@ref(fig:megaPlot) shows estimates for each of the single-predictor models along with the additive linear model. The additive model revealed estimates that were highly comparable to the single-predictor model.  

In summary, we find that predicate type is a significant predictor of the effect size: conditions with transitive sentences were associated with larger effect sizes than those tested with intransitive sentences. No other theoretical variable significantly moderated the syntactic bootstrapping effect.


## Methodological Moderators 

One limiting factor in interpreting the moderating  role of theoretical variables is that there was appreciable variability across studies in the exact method used in testing children. It is possible that this methodological variability conceals true underlying moderating influences. For example, if researchers adapt their method to the age of the children they are targeting,  developmental change in the strength of the effect may not be detectable [@bergmann2018promoting]. 

To evaluate this possibility, we asked whether the five different methodological variables (practice phase, sentence repetitions, character identification phase, synchronicity, testing procedure structure) moderated the syntactic bootstrapping effect. None of these methodological variables were significant moderators of the effect in a single predictor model (Fig. \@ref(fig:megaPlot); see SI, Sec.3 for exact estimates). In an additive linear model with all five methodological predictors, there was a marginally significant effect of testing procedure structure ($\beta$ = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_p_print`), with mass testing designs tending to have larger effect sizes than distributed designs. This finding suggests that children tested in a procedure with only one train-test pair performed better than those tested in procedure with multiple train-test pairs. Finally, we asked how these methodological moderators related to our theoretical moderators of interest. Controlling for methodological variables did not qualitatively change the role of any of the theoretical moderators (see SI, Sec.5).  Taken together, these analyses suggest that methodological variables do not play a large influencing role on the size of the syntactic bootstrapping effect. 

<!-- rather than list all the (null, not-very-interesting) estimates in the main text, lets add tables to the SI with the exact values from the theoretical and methodological plots --> 
