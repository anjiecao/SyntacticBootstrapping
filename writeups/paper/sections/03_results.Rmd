```{r generate to-print model results}
MODERATORS <- c( "NULL", "mean_age","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type","agent_argument_number", "n_repetitions_sentence", "n_repetitions_video", "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "visual_stimuli_pair", "test_method","presentation_type_collapsed","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )
mod_print <- generate_moderator_df(MODERATORS, ma_data)
```

```{r label = "forest", fig.pos = "!t", fig.cap = "wow",fig.width = 45, fig.height = 60}
#fig.pos = "!t", 
generate_forest_plot(ma_data)
```

Figure\ \@ref(fig:forest) shows effect size estimates for all conditions in our sample. The weighted mean effect size was `r filter(mod_print, moderator == "NULL")$estimate_print`, which significantly differed from 0  (*Z* = `r filter(mod_print, moderator == "NULL")$z_print`; *p* `r filter(mod_print,  moderator == "NULL")$p_print`). There was evidence for considerable heterogenity in effect sizes across our sample (*Q* = `r filter(mod_print, moderator == "NULL")$Q_print`; *p* `r filter(mod_print, moderator == "NULL")$Qp_print`).

## Evidential value of the syntactic bootstrapping literature

We first evaluated the evidential value of the literature by assessing the evidence for publication bias. Publication bias refers to the tendency to selectively publish positive findings. The intuition underlying these analyses is that a meta-analysis includes both studies that have statistically significant effect sizes for the target phenomenon and studies those do not. Critically, due to publication pressures, there are likely some studies that have been conducted that are not stastically significant but are not part of our meta-analysis. The absence of these studies from the meta-analysis leads to a meta-analytic estimate that over-estimates the true effect size, and threatens the evidential value of the literature. We present two analyses of publication bias: a classic funnel plot analysis, and a sensitivity analysis that assumes a more plausible model of publication bias. 

```{r label = "funnel", fig.pos = "t!", fig.height = 3.75, fig.width = 4, fig.cap = "Funnel plot showing the standard error of each effect size estimate in our meta-analysis as a function of the magnitude of that effect size. The gray and red vertical dashed lines correspond to an effect size of zero and the meta-analytic effect size estimate, respectively. The grey funnel represents a 95% confidence interval around meta-analytic estimate. In the absence of publication bias, effect size estimates should by symmetrically distributed around the red line."}
generate_funnel_plot(ma_data)

reg_result <- rma.mv(d_calc ~ sqrt(d_var_calc),  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=ma_data) # access zval and pval from here
reg_z <- round(reg_result$zval, digits = 2)
reg_p <- if_else(reg_result$pval < 0.0001, "< .0001", as.character(reg_result$pval))
#reg_result
```

<!-- [DONE?]the above method of calculating egger's test does not take into account the clustered structure of our data, as we do elsewhere. There's no way to add this structure using the regtest function; you'll need to do the manual regression method that I added to the markdown on publication bias to get the correct values. [DONE] The red line on the funnel itself should also reflect the model estimate from the correct ma model. -->

Figure\ \@ref(fig:funnel) presents the funnel plot for the effect sizes in our sample. A funnel plot shows  estimates of effect size variance (plotted with large values lower on the axis) as a function of the magnitude of the effect size [@egger1997bias]. Under a model of publication bias in which researchers decide whether or not to publish a study based on the magnitude of its effect size (larger effect sizes being more likely), effect size estimates should fall symmetrically around the grand effect size estimate. Evidence of asymmetry around the grand mean, particularly more large, positive effect sizes, would suggest that the literature reflects a biased sample of studies. A formal test of asymmetry in our sample revealed evidence for asymmetry (Egger's test: *Z* = `r reg_z`; *p* `r reg_p`).

The funnel plot analysis provides some evidence for publication bias, but the interpretation of this analysis is limited by the fact that it assumes a relatively implausible model of how researchers decide which studies to make public: the criteria for publishing a study in a journal is typically not the *size* of the effect, as assumed by the funnel plot analysis, but rather whether or not the p-value of the hypothesis test for that effect is below some threshold (usually .05). We therefore conducted a second analysis of publication bias, called a sensitivity analysis [@mathur2020sensitivity], which assumes that the decision to publish results is determined by the size of the p-value, rather than the magnitude of the effect size.

<!-- I took out the bit about hetergenity since there are ways to address hetergenity using funnel plots - i.e. just include them in the model-->

The goal of the sensitivity analysis is determine how sensitive the meta-analytic effect size is to "missing" non-significant studies. Critically, because the degree of publication is not known (i.e. hthe degree to which significant results are more likely to be published, relative to insignificant results), the sensitivity analysis assumes a worst-case publication bias scenario and estimates the meta-analytic effect size under this scenario. The worst-case scenario assumed by the model is that significant studies are infinitely more likely to be published than non-significant studies ^[Technically, the model assumes studies with effect sizes that are statistically significant (*p* < .05) *and* greater than zero are infintely more likey to be published. See Mathur and VanderWeele (2020) for additional details.]. A meta-analytic effect size under this scenario can be estimated by analyzing only those studies with significant effect size estimates.

```{r}
ma_data_with_affirm <- ma_data %>%
  mutate(pvalue =  2 * (1 - pnorm( abs(d_calc / sqrt(d_var_calc)))),
         affirm =  (d_calc > 0) & (pvalue < 0.05))

affirm_model<- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/x_1, data=
          ma_data_with_affirm %>% filter(affirm == FALSE)) 

worst_case_estimate_print <- paste0(as.numeric(round(affirm_model$beta, 2)),
                                    " [",
                                    as.numeric(round(affirm_model$ci.lb, 2)),
                                    ", ",                                                                                                        as.numeric(round(affirm_model$ci.ub, 2)),
                                    "]"
)
```

Conducting this sensitivity analysis on our data reveals that no amount of publication bias could attentuate the point estimate of the effect size to 0. Nevertheless, the worse-case scenario appreciably attenuates the meta-analytic effect size, and the attenuated effect size estimate includes 0 in its 95% confidence interval (`r worst_case_estimate_print`; see SI for additional details). 

In sum, across two types of analyses, we find some evidence for publication bias in the synactic bootstrapping literature, but even under worst-case scenarios publication bias was not enough to fully attenuate the meta-analytic point estimate to 0. Further, some of the publication bias observed in the funnel plot analysis may be due to heterogeneity in the data. In the following sections, we analyze theoretical and methodological moderators that could be contributing to this heterogeneity, though we emphasize that the likely presence of publication bias implies that these moderators should be interpreted with caution. 

## Relating the syntactic bootstrapping effect to other word learning stategies
<!-- TO DO -->

## Theoretical Moderators

We next asked whether the overall effect size estimate was moderated by our theoretical moderators of interest: development-related moderators (vocabulary and age), and sentence structure moderators (predicate and noun phrase types).

<!-- Things to change in the plot: (1) add caption, making sure to (briefly) explain how to intepret the factor levels and why vocab is not included in the plot (2) change x axis label to match the labels of other plots (3) full model should have lines connecting point estimates to show that they are all simultaneously in the same model, (4) put factor levels in parentheses (i.e. "(Prounoun/noun)"), (5) Change legend to be "Model Type: Full; Single Predictor" or something like that. (6) compress the plot veritically so there's less wasted space? (7) let's combine this plot with the method one so they are side by side. -->
```{r label = "mega_plot"}
theoretical_mod <- rma.mv(d_calc ~ sentence_structure + agent_argument_type + mean_age, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/x_1,
                     method = "REML",
                     data = ma_data) 

theoretical_mod_print <- generate_mega_model_df(theoretical_mod)
#theoretical_mod_print
#summary(theoretical_mod)
```

```{r label = "theoretical_moderators_plot", fig.pos = "T!",  fig.cap = ""}
generate_predictor_plot(mod_print, theoretical_mod_print, "theoretical")
```


### Development
```{r}
age_vocab_cor <- cor.test(ma_data$mean_age,ma_data$productive_vocab_median)
cor_p <- ifelse(round(age_vocab_cor$p.value, digits = 2)<0.0001, "< 0.0001", as.character(round(age_vocab_cor$p.value, digits = 2)))
```

<!-- Things to change in the plot: (1) add caption, (2) change x axis to be months (3) I know we've gone back and forth about this, but let's just include the simple lm model fit. I think having both is confusing. -->
```{r, label = "development"}

age_model <- rma.mv(d_calc ~ mean_age, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/x_1,
                     method = "REML",
                     data = ma_data) 

predicted_val_age <- predict(age_model, addx = TRUE) %>% 
  as.data.frame() %>% 
  rownames_to_column("data_frame_id") %>%
  select(data_frame_id, pred) %>%
  left_join(ma_data %>% rownames_to_column("data_frame_id") %>%
              select(mean_age, d_calc, data_frame_id)) %>%
  mutate(model_type = "age") %>%
  rename(developmental_predictor = mean_age)

predicted_val_age %>%
  ggplot() +
  geom_point(aes(x = developmental_predictor, y = d_calc)) +
  geom_smooth(method = "lm",aes(x = developmental_predictor, y = d_calc)) +
  theme_classic() + 
  geom_smooth(method = "lm",
              aes(x = developmental_predictor, y = d_calc,color = "Cohen's d"),
              ) +
  geom_smooth(method = "lm",
              aes(x = developmental_predictor, y = pred,
                  color = "Model's Predicted Values")) +
  xlab("Mean Age (Days)") + 
  ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))   +
  scale_color_manual(name = "Regression Line",             # legend name
                     values = c("Cohen's d" = "blue",  # map regression line colors
                                "Model's Predicted Values" = "red"))
```

Does the syntactic bootstrapping effect get stronger across development? We examined two measures of developmental change: age and vocabulary size. These two measures were strongly correlated with each other (*r*(`r age_vocab_cor$parameter`) = `r round(age_vocab_cor$estimate, digits = 2)`, *p*: `r cor_p`). Neither age ($\beta$ = `r filter(mod_print, moderator == "mean_age")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "mean_age")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "mean_age")$mod_z_print`, *p* `r filter(mod_print, moderator == "mean_age")$mod_p_print`; Fig.\ \@ref(fig:development); \@ref(fig:mega_plot)), nor vocabulary size ($\beta$ = `r filter(mod_print, moderator == "productive_vocab_median")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_z_print`, *p* `r filter(mod_print, moderator == "productive_vocab_median")$mod_p_print`) significantly moderated the effect.

### Sentence structure 
```{r summarize_sentence_structure}
ss_summary <- ma_data %>% 
  group_by(sentence_structure) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )
```
<!-- throughout, let's change what we have been calling "sentence structure" to be "predicate type" and use sentence structure to refer to both predicate type and noun phrase type --> 

We next asked how properties of the sentence struture that children heard influenced the strength of the syntactic bootstrapping effect. Predicate type (transitive vs. intransitive) significantly was a significant moderator, ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "sentence_structure")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "sentence_structure")$mod_z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$mod_p_print`), with the effect being larger for transitive conditions (*M* = `r filter(ss_summary, sentence_structure == "transitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "transitive")$sd`) relative to intransitive conditions (*M* = `r filter(ss_summary, sentence_structure == "intransitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "intransitive")$sd`). In contrast, there was no effet of agent argument type (pronoun vs. noun; $\beta$ = `r filter(mod_print, moderator == "agent_argument_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "agent_argument_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "agent_argument_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "agent_argument_type")$mod_p_print`). 

To compare the effects of all theoretical moderators, we fit an additive model with all theoretical variables as fixed effects. We excluded vocabulary size from the additive model since it was highly correlated with age, and only available for a subset of conditions (`r  num_vocabulary_available`).  \@ref(fig:mega) shows estimates for each of the single-predictor models along with the additive linear model. The additive model revelaed estimates that were highly comproable to the single-predictor model.  

In summary, we found that predicate type is a significant predictor of the effect size. Conditions with transitive sentences were associated with larger effect sizes than those tested with intransitive sentences. No other theoretical variable significantly moderated the effect.


## Methodological Moderators 

One limiting factor in interpetting the moderating  role of theoretical vairables on the syntactic bootstrapping effect is that there was variability across studies in the exact method used in testing children. It is possible that this methodological variability conceals true underlying moderating influences. For example, if researchers adapt their method to the age of the children they are targeting, this might conceal developmental change in the strength of the effect [@bergmann2018promoting]. 

```{r mega model for methodological moderators, message=FALSE, warning=FALSE}
methodological_mod <- rma.mv(d_calc ~ character_identification + practice_phase + presentation_type_collapsed + test_mass_or_distributed + n_repetitions_sentence,
                             V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/x_1,
                     method = "REML",
                     data = ma_data)



methodological_mod_print <- generate_mega_model_df(methodological_mod)
#methodological_mod_print
#summary(methodological_mod)
```

<!-- combine this plot with the theoretical plot, side by side. make same changes as to theoretical plot -->
```{r label = "methodological_mega_model", fig.pos = "T!", fig.cap = ""}
generate_predictor_plot(mod_print, methodological_mod_print, "methodological")
```

We next asked whether the five different methodological variables (practice phase, sentence repetitions, character identifcation phase, synchronicity, testing procedure structure) moderated the syntactic bootstrapping effect. None of these methodological variables were significant moderators of the effect in a single predictor model (Fig. \@ref(fig:mega); see SI for exact estimates). In an additive linear model with all five methodological predictors, there was a significant effect of testing procedure structure ($\beta$ = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_p_print`), with distributed testing designs tending to have larger effect sizes than mass designs. This finding suggests that children tested in a procedure with only one train-test pair performed better than those tested in procedure with multiple train-test pairs. Finally, we asked how these methodological moderators related to our theoretical moderators of interest. Controlling for methodological variables did not qualitiatively change the role of any of the theoretical moderators (see SI) .  Taken together, these analyses suggest that methodological variables do not play a large influencing role on the size of the syntactic bootstrapping effect. 

<!-- rather than list all the (null, not-very-interesting) estimates in the main text, lets add tables to the SI with the exact values from the theoretical and methodological plots --> 
