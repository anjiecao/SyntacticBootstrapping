---
title             : "A meta-analysis of the syntactic bootstrapping phenomenon in word learning"
shorttitle        : "Syntactic Boostrapping MA"

author:  
  - name          : "Anjie Cao"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "what is this" # <- you're physical address at Stanford (something something Jordan Hall - it's a weird relic of when people actually sent stuff in snail mail)
    email         : "acao@andrew.cmu.edu" # <- probably want your new stanford email
  - name          : "Molly Y. Lewis"
    affiliation   : "2"
    email         : ""
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Department of Psychology, Carnegie Mellon Unviversity"
    
author_note: |
  okie

abstract: |
  so abstract!
  
keywords          : "language acquisition, syntactic bootstrapping, meta-analysis"
wordcount         : ""

bibliography      : ["references.bib"]
csl               : "apa6-meta.csl"

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}
  - \usepackage{hyphsubst}
  - \floatplacement{figure}{t!} # make every figure with caption = t

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
#lang              : "english"
documentclass     : "apa6"
class             : "man"
output:
  #always_allow_html: true
  papaja::apa6_pdf:  #word_document 
    latex_engine: xelatex # this solved an encoding issue
    includes: 
      after_body: appendix.tex
---

```{r load_utility_packages, include = FALSE}
library(papaja)
library(rmarkdown)
library(tidyverse) 
library(here)
library(glue)
library(metafor)
library(knitr)
library(PublicationBias)
library(janitor)

source(here("writeups/paper/scripts/prisma_diagram.R"))
source(here("writeups/paper/scripts/model_print.R"))
source(here("writeups/paper/scripts/forest_plot_helper.R"))
source(here("writeups/paper/scripts/funnel_plot_helper.R"))


```


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = FALSE,
                      echo = FALSE,
                      fig.pos = "t!")
theme_set(theme_classic())
```

```{r constant}
alpha <- .05
CRIT_95 <- 1.96
CRIT_99 <- 2.58
```

```{r read in data}
DATA_PATH <- here("data/processed/syntactic_bootstrapping_tidy_data.csv") 
RAW_DATA_PATH <- here("data/raw/syntactic_bootstrapping_raw_data.csv")

ma_data <- read_csv(DATA_PATH)
```

# Introductions 
Children build up their vocabularies at a dazzling speed. Toward the end of the second year, they were estimated to have a productive vocabulary size of around 300 [@fenson1994variability]. But children’s impressive word-learning abilities do not apply equally to words of all kinds. Verbs, for example, constitute a special challenge for children. Numerous observational studies have shown that verbs are often learned later than nouns [@nelson1973structure; @goldin1976language; @lieven1992individual; @longobardi2017early; @nelson1993nouns], and laboratory experiments also found that children’s verb learning is more slowly and difficult than noun learning[@gentner1978relational;@childers200612;@schwartz1984words; @oviatt1980emerging;@childers2012children;@imai2008novel;@abbot2017role;@childers2002two].  Some scholars have argued that the challenge of verbs is language-dependent [@choi1995early; @tardif1996nouns; @tardif1999putting]. Nevertheless, the cross-linguistic findings are mixed, with others found consistent patterns across languages [@au1994input; @papaeliou2011vocabulary; @bornstein2004cross;@kim2000early].  A more recent large scale, cross-linguistic corpus analysis has shown that across 23 different languages, verbs and other predicates, when compared with nouns, constitute a smaller proportion of children’s early production vocabularies, though there are more cross-linguistic differences in early comprehension vocabularies [@frank2019variability]. Together, these findings suggest that at least certain aspects of the challenge associated with verb learning are universal across languages. Some have hypothesized that it is due to the nature of verbs’ references: Unlike nouns, the references of verbs are ephemeral and ever-changing. As a consequence, to learn verbs, children need to rely less on the unreliable extralinguistic information and rely more on the information within the verbs’ linguistic context, such as syntax[@gentner2001individuation, @gentner2006verbs].   

The idea that syntactic information can shape children’s interpretations of novel words was not new. As early as the late 1950s, a classic study by @brown1957linguistic has shown that 3- to 5-year-old children would choose to map a novel word to an action if the word is introduced in a verb context (e.g.“Do you know what it means to sib?”), and map to an object if introduced in a noun context(e.g.“Do you know what a sib is?”). However, this early finding only provided evidence for older children’s abilities to discriminate between syntactic categories. It remained unknown how early the abilities emerge and how detailed the underlying representations for the verbs are. In the late 1980s and early 1990s, a hypothesis known as “Syntactic Bootstrapping” revived the interests in the early interaction between syntax and verbs’ semantics during language development [@gleitman1990structural; @Landau1985]. The basic idea of Syntactic Bootstrapping is that the structures of the sentences were potent cues for children to interpret the meanings of the verbs. This idea soon received empirical support from a seminal study by @naigles1990children. By using the Intermodal Preferential Looking Paradigm [@golinkoff1987eyes], @naigles1990children provided the first evidence for 25-month-olds children’s abilities to incorporate syntactic cues in verbs learning. 

In this seminal study, the children were tested on four different novel verbs. For each novel verb, there was a training phase and a testing phase. During the training phase, the children would hear either a series of transitive sentences or intransitive sentences, depending on the conditions they were assigned to. Each sentence grammatically contained the novel verb (e.g., for transitive: ”The duck is gorping the bunny”; for intransitive: “The duck and the bunny are gorping”.) The children were also exposed to the visual stimuli depicting a two-actor action while listening to these sentences. The two actors, one in a bunny suit and the other in a duck suit, would perform two novel actions simultaneously. They would move their arms synchronously while the one pushed the other bending forward. After the training phase, the children would be prompted to look for the action (e.g. “Where’s the gorping now?”). The children faced two screens side-by-side, one would show the bunny and duck performing non-causal synchronous arm movements, while the other would show the causal pushing-and-bending action. The children’s fixation time toward each of the screens was recorded. A longer looking time for either of the screen indicated a match between the children’s conceptual representations of the novel verbs and the action on the screens. @naigles1990children found that children assigned to the transitive condition looked longer at the causal action than the non-causal action, whereas the children in the intransitive condition showed the opposite pattern. This indicated that at this age children can interpret the meanings of novel verbs as informed by the syntactic structures in a surprisingly detailed way.

This paradigm soon became a standard in studying Syntactic Bootstrapping in young children. Numerous experiments have adopted and extended this paradigm to explore different facets of Syntactic Bootstrapping. Many variations of this paradigm were developed: using pointing instead of looking as the behavioral responses measured [e.g., @arunachalam2010meaning; @kline2017linking; @rowland2010role]; using human actors as the protagonists of the actions instead of the humans in animal suits [e.g., @gertner2012predicted; @bunger2006constrained; @messenger2015learning]; adding a practice phase or a character-identification phase to reduce the task demands for young children [@scott2009two;@gertner2012predicted;@scott2017lookit]. The changes can potentially bring unintended effects on the learning outcomes: studies differ significantly in the amount of training experience provided to the participants. Some studies, similar to the original @naigles1990children design, distributed the training experience across multiple different novel verbs [e.g., @he2020two; @arunachalam2013two; @naigles1993first; @naigles1996use]. Others, by the contrary, focused on training and testing on one novel verb throughout the experiment [e.g., @jin2015role;  @kline2017linking;@messenger2015learning]. As a result, children may have heard the novel verb repeated from as little as 3 times [e.g. @naigles1990children; @naigles1993first; @naigles1996use], to as many as 27 times [e.g., @arunachalam2013two] before they were tested. There are also changes motivated by theoretical considerations. For instance, whether the syntactic cues alone were sufficient to support verbs learning has been studied by altering the visual stimuli during the training phase. Rather than seeing the visual stimuli representing the relevant actions, the children in some training phases would see two actors talking to each other or one actor talking over the phone [e.g. @yuan2009really]. The absence of extralinguistic cues did not prevent the children from interpreting the novel verbs correctly, which further suggests the significance of the syntactic cues in verb learning at an early age.  

The changes in paradigms have also led to some inconsistent findings. For example, The results have been mixed on whether the children have a transitivity bias, that is, children are more likely to match the correct scenes when the novel verbs are embedded in the transitive frames. Some experiments found that children who were exposed to the transitive sentences would show a larger preference to the matching scenes, whereas children exposed to the intransitive sentences looked by chance or showed a looking pattern similar to the control group who were not provided with any relevant syntactic cues [@yuan2012counting; @arunachalam2010meaning]. This transitivity advantage can be attributed to three factors: the clarity, the experience, and the perceptual saliency of the stimuli. First, under certain contexts, the transitive sentences have less ambiguity than their intransitive counterparts.  If one sees two scenes: in one a girl patting a boy, in the other a girl and a boy jumping side-by-side, then upon hearing “the girl is gorping the boy”, only the scene with patting action is a plausible interpretation. In contrast, if one hears the intransitive sentence, “the girl is gorping”,  the verb can be interpreted as being consistent with both the causative scene and the non-causative scene. Second, children may have more experience with transitive sentences than intransitive sentences. A corpus analysis on parental utterances for 1- to 6-year-old children found that transitive sentences make up approximately 24.36% of all utterances, whereas intransitive sentences occupy only 17.24% [@laakso2007pronouns]. Thus the familiarity with transitive sentences can potentially improve children’s understanding of transitive sentences in the lab. Last but not least, the perceptual saliency of the visual scenes may have contributed to the asymmetrical performance in the transitive conditions and the transitive conditions. Children were found to have a baseline preference for two actors event over one actor event [e.g., @yuan2012counting], and for synchronous movement over causative movement [e.g., @naigles1993first]. These preferences can make detecting an effect in the transitive condition easier than in the intransitive condition. 

However, other studies have found the opposite pattern. In the second and the third experiment in @naigles1993first, it was children in the transitive conditions who showed no preference for neither of the events, whereas children in the intransitive conditions looked longer at the matching event. Similarly, @bunger2004syntactic also showed that children exposed to the intransitive sentences tend to look longer at the matching scene but those exposed to the transitive sentences look by chance.  There were two proposed explanations for this reversed effect. First, transitive verbs can be more ambiguous under certain contexts due to children’s sensitivity to the internal structure of the event. Research has shown that children under 1 year of age already possess impressive abilities to detect the internal structure of an event. Following the structure, they can parse the event into subcomponents[@hespos2009infants; @hespos2010infants; @stahl2014infants; for a recent review, see @levine2019finding]. On the one hand, successful parsing underlies successful verb learning [@friend2011beyond]. But on the other hand, this sensitivity to the internal structure of the event can also lead to challenges for learning transitive verbs. As @bunger2004syntactic pointed out, the transitive verbs can denote both the means of the action and the results of the action. In comparison, the possible meanings of intransitive verbs are more constrained to the results of the action. The wider range of possible meanings could make it more difficult for the children to correctly match the sentences and the scenes. The second possibility for this asymmetry is the processing demands brought by the words in the argument positions. @lidz2009one noted that the minimal demand from a simple transitive sentence is for children to map at least two nouns, the subject and the object to the corresponding agent and the patient. But for an intransitive sentence, the demand can be as low as only mapping only the subject of the sentence to the agent of the scene. In consequence, learning the novel verbs in the intransitive sentences can be easier than in the transitive sentences. 

Besides the number of words in the verbs’ arguments, the semantic content is also considered to be a key moderator for the learning process. Some scholars have proposed that semantically rich contexts are beneficial for verb learning, because the semantic content of the surrounding nouns can scaffold the interpretation of the verbs, together with the syntactic structure [@arunachalam2011grammatical; @arunachalam2015let; @gillette1999human; @piccin2007nouns; @gleitman2005hard; @fisher1994better; @imai2008novel]. But the empirical support for this view is mixed. In a recent study, @he2020linguistic manipulated the amount of semantic content in the subject argument. Some preschoolers were provided with more information about the subject (“The tall girl is fezzing”), and others less (“The girl is fezzing”). Contrary to the semantic-scaffolding view, @he2020linguistic found that the children learned better when the nouns were not modified, indicating that the children’s limited information processing abilities may have impeded them from utilizing the semantic content. This pattern also persists when the number of words in the agent argument is controlled for. Pronouns have less semantic content than concrete nouns. Multiple studies have found that preschool children learn verbs better when the verbs co-occur with pronouns instead of nouns. They are not only more likely to correctly identify the scenes corresponding to the linguistic stimuli but also are more likely to generalize the syntactic frames to new verbs [@lidz2009one; @childers2001role].

To further shed light on these conflicting findings, we decided to conduct a meta-analysis on syntactic bootstrapping literature. Our first goal is to estimate the robustness of this phenomenon by calculating the meta-analytic effect size. Knowing the effect size is consequential to theory building [@lewis2016quantitative; @bergmann2018promoting]. Many past meta-analyses have tapped into other domains of early language development such as phonotactic learning [@cristia2018can], word segmentation [@bergmann2016development], and mutual exclusivity [@lewis2020role]. To our knowledge, this is the first meta-analysis examining the development of syntactic abilities in early childhood and its influence on word learning. Our second goal is to examine the potential moderators of the effect. Understanding the moderators can be valuable for both theoretical reasons and practical reasons. Due to the limited sample sizes, individual infant experiments often have insufficient statistical power, which can make detecting relevant moderators difficult [@oakes2017sample]. The variations among different testing procedures also made it difficult to reconcile the conflicting findings, because the differences in the observed effect can be caused by a combination of methodological factors, rather than reflecting the differences in the measured constructs. The meta-analytic approach has the opportunity to reveal some of the underlying interactions between different factors, which can inform future researchers about their experimental designs. Last but not least, our fourth goal is to evaluate how the effect changes with age. Most of the syntactic-bootstrapping studies used a cross-sectional design. Despite the virtue of being rigorously controlled, the “time-slice” nature of this design made it difficult to reconstruct a continuous developmental trajectory. The meta-analysis can provide unique insights into how children’s abilities to incorporate syntactic information into verbs learning to develop in the first three years of life. 

# Methods

## Literature Search  

```{r literature search - search procedure}
records_identified_google_scholar <- 2330
records_identified_forward_search <- 1009
records_identified_review_reference <- 155
records_identified_experts_in_the_field <- 11
total_records_identified <- records_identified_google_scholar + records_identified_forward_search + records_identified_review_reference + records_identified_experts_in_the_field

records_screened_google_scholar <- 10 * 60 
records_screened_forward_search <- 10 * 10 
records_screened_review_reference <- 155 
records_screened_experts_in_the_field <- 14

# Yikes this code was hard to read! Line breaks are your friend! :) Minimally put a line break after each pipe. 
total_records_screened <- records_screened_google_scholar +
  records_screened_forward_search + 
  records_screened_review_reference + 
  records_screened_experts_in_the_field

# class project + else
full_text_assesed_for_eligibility <- 503 + 166
final_inclusion <- read_csv(RAW_DATA_PATH) %>% 
  filter(paper_eligibility == "include") %>%
  distinct(unique_id) %>%
  count()

screen_exclusions <- total_records_screened - full_text_assesed_for_eligibility
full_text_assesed_exclusion <- full_text_assesed_for_eligibility -
  final_inclusion
```

```{r prisma, fig.cap = "PRISMA plot showing..."}
prisma2(found = format(records_identified_google_scholar + records_identified_forward_search, big.mark = ","), 
        found_other = records_identified_review_reference + records_identified_experts_in_the_field,
        no_dupes = total_records_screened, 
        screened = total_records_screened, 
        screen_exclusions = total_records_screened - full_text_assesed_for_eligibility, 
        full_text = full_text_assesed_for_eligibility,
        full_text_exclusions = full_text_assesed_exclusion, 
        quantitative = final_inclusion,
        font_size = 10,
        dpi = 50
      )
```

<!-- italicize in text numbers with $N$; can reference figures in text with chunk name, e.g. Figure\ \@ref(fig:prisma) -->
We conducted our literature search following the  Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist [PRISMA; @moher2009]. We identified relevant papers by conducting a keyword search in Google Scholar with the phrase "Syntactic Bootstrapping" and a forward search on papers that cited the seminal paper [@naigles1990children] (total records identified: *N* =  `r records_identified_google_scholar + records_identified_forward_search`; retrieved between May 2020 and July 2020; Figure\ \@ref(fig:prisma)). We screened the first $60$ pages (*N* = $600$) of the keyword search results and the first $10$ pages (*N* = $100$) of the forward search results. The screening processes ended because we could no longer identify relevant, non-duplicate papers from consecutive pages. Additional papers were identified by consulting the references section of the most recent literature review (*N* = `r records_identified_review_reference`) [@fisher2020developmental] and the experts in the field (*N* = `r records_identified_experts_in_the_field`). In our final sample, we included published journal articles, conference proceedings, doctoral dissertations, and unpublished manuscripts. They will be collectively referred to as "papers" in the following sections. Each paper may include multiple experimental conditions, and thus provides multiple effect sizes for the final analysis.

We restricted our final sample to papers that satisfied the following conditions: First, the experimental paradigm involved a two-alternative forced-choice task, in which the participants were instructed to identify the scene that matched the linguistic stimuli. Second, the visual stimuli were two events displayed side-by-side on a computer monitor. The two events must include one depicting causative action (e.g. one agent causes the other to move), and one non-causative action (e.g. two agents move simultaneously but do not causally interact with each other). The media displayed can be either videos or animation clips. Third, the linguistic stimuli included at least one novel verb embedded in a syntactically informative frame. For example, "Look, it's kradding!" embeds the novel verb in an intransitive syntactic frame that is informative about the meaning of the novel verb "kradding", while "Look, Kradding!" does not and therefore would be excluded. Finally, we resricted our sample to English-speaking, typically-developing children.
  
```{r literature search - report final sample}
n_effect_sizes <- ma_data %>%
  filter(!is.na(d_calc)) %>%
  nrow()

n_papers <- ma_data %>%
  distinct(unique_id) %>%
  nrow()

unique_infants <- ma_data %>% 
  filter(unique_infant == "unique_condition") %>% 
  filter(adult_participant == "no") %>% 
  summarize(sum_infants = sum(n_1))

non_unique_infants <- ma_data %>% 
  filter(unique_infant == "not_unique") %>% 
  filter(adult_participant == "no") %>%
  distinct(short_cite, expt_condition, .keep_all = TRUE) %>% 
  summarize(sum_infants = sum(n_1))

different_infants <- unique_infants + non_unique_infants
mean_age_day <- ma_data %>% 
  filter(!is.na(mean_age)) %>%
  summarize(mean_age =  format(round(mean(mean_age), 2)))

```
  
<!-- There's a bunch of missing values in the pdf, not sure why--> 
   Our final sample included data from `r different_infants` unique infants (Mean age: `r mean_age_day` Days), reported in `r n_effect_sizes` individual effect sizes from `r n_papers` individual papers. 
   
## Data Entry

For each paper, we entered the paper metadata (e.g., citation), information to calculate effect sizes, and moderators. We entered a separate effect size for each experimental condition and age group in paper. Most papers therefore contained multiple effect sizes.

```{r data_source_summary}
data_source_summary <- ma_data %>% 
  group_by(data_source_clean) %>% 
  count()

num_author_contact <- data_source_summary %>% 
  filter(data_source_clean == "author_contact") %>% 
  pull(n)

num_plot <- data_source_summary %>% 
  filter(data_source_clean == "plot") %>% 
  pull(n)

num_text_or_table <- data_source_summary %>% 
  filter(data_source_clean == "text/table") %>% 
  pull(n)

num_imputed <- data_source_summary %>% 
  filter(data_source_clean == "imputed") %>% 
  pull(n)
```

In order to calculate the effect sizes, we recorded the sample size of each condition, the group mean, and the across-participant standard deviation. The mean and standard deviation were obtained from one of the four ways: a) directly retrieved from the results section or the data-presenting tables (*N* = `r num_text_or_table`); b) recovered from the plots by measuring the height of the bars and the error bars (*N* =`r num_plot`); c) contacted the original authors (*N* = `r num_author_contact`); d) imputed using values from studies with similar designs (*N* = `r num_imputed`; Among the `r num_imputed`, missing standard deviation from [@hirsh1996young] were imputed with values from [@naigles1990children]; missing values from [@arunachalam2013out] were imputed with values from [@arunachalam2013two]). We decided to impute the missing standard deviations instead of excluding the studies because past work has shown that using "borrowed values" is beneficial to the accuracy of meta-analysis [@furukawa2006imputing].

For looking time studies, when the paper only reported the raw looking time in seconds, we calculated the proportion of correct response by dividing the mean looking time toward the matching scenes by the sum of mean looking time toward the matching scenes and the mean looking time toward the non-matching scenes (i.e., excluding the look away time from the denominator). The raw standard deviations were also converted to the corresponding values by being divided by the sum. 

```{r categorize_by_proto}
num_proto_approach <- ma_data %>% 
  filter (inclusion_certainty == 2) %>% 
  count()

num_atypical_approach <- ma_data %>% 
  filter (inclusion_certainty == 1) %>%
  count()
```

<!-- We classified our final samples of effect sizes (N = `r n_effect_sizes`) into two categories: the prototypical ones (N = `r num_proto_approach`) and atypical ones (N = `r num_atypical_approach`). Prototypical studies were the ones in which the set-up and the procedures were most similar to the seminal paper [@naigles1990children]. The atypical studies were experiments that met all of the inclusion criteria, but deviated from the prototypical designs in some non-trivial ways (The stimuli involved more complex syntactic structure or event structure: N = `r n_effect_sizes - 12`; The testing procedure included a contrast phase: N = 10; The experiment was conducted through online platform: N = 2). To preserve the homogeneity of the experiments without reducing the power of our analysis, we conducted our statistical analysis both on the entire sample and specifically on the prototypical studies. -->

```{r vocb_available}
num_vocabulary_available <- ma_data %>%
  filter(!is.na(productive_vocab_median)) %>% count()
```


At the participant level, we coded infants' mean age in days (*N* = `r n_effect_sizes`) and the median productive vocabulary measured by MacArthur-Bates Communicative Development Inventories (CDI) Words and Sentences [@fenson2000short] (*N* = `r  num_vocabulary_available`). 



```{r categorize_by_linguistic_stimuli}
num_transitive <- ma_data %>% filter(sentence_structure == "transitive") %>% count()
num_intransitive <- ma_data %>% filter(sentence_structure == "intransitive") %>% count()

num_agent_sum <- ma_data %>% count(agent_argument_type)
num_agent_noun <- num_agent_sum %>% filter(agent_argument_type == "noun") %>% pull(n)
num_agent_pronoun <- num_agent_sum %>% filter(agent_argument_type == "pronoun") %>% pull(n)

num_patient_sum <-  ma_data %>% count(patient_argument_type)
num_patient_noun <- num_patient_sum %>% filter(patient_argument_type == "noun") %>% pull(n)
num_patient_pronoun <- num_patient_sum %>% filter(patient_argument_type == "pronoun") %>% pull(n)
num_patient_intransitive <- num_patient_sum %>% filter(patient_argument_type == "intransitive") %>% pull(n)

#num_test_agent <- ma_data %>% filter(test_type == "agent") %>% count()
#num_test_action <- ma_data %>% filter(test_type == "action") %>% count()
```

At experimental condition levels, we coded moderators that are theoretically relevant and methodologically relevant. The information was either retrieved from the methods section of the paper or obtained via contacting authors. First, for the theoretically relevant moderators, we coded features of the linguistic stimuli during both training and testing, including the sentence structure and the type of words used in the agent arguments.  Sentence structure has two levels, transitive (*N* = `r num_transitive`) and intransitive (*N* = `r num_intransitive`). The training linguistic stimuli were categorized as transitive if the novel verbs were embedded in a sentence with two or more noun arguments, and intransitive if the novel verbs were embedded in a sentence with one noun argument. We also coded the types of words used in the agent argument (Noun: *N* = `r num_agent_noun`; Pronoun: *N* = `r num_agent_pronoun`). If in one condition the linguistic stimuli contained at least one occurence of pronouns in the agent argument, then the condition would be coded as "pronoun". Those with no pronoun throughout were coded as "noun". We also coded word types used in the patient argument following the same rule (Noun: *N* = `r num_patient_noun`; Pronoun: *N* = `r num_patient_pronoun`). Because this only applies to studies with transitive structure (*N* = `r num_transitive`), we only presented the exploratory analysis in the SI. 


```{r categorize_by_visual_stimuli}
num_modality_sum <- ma_data %>% count(stimuli_modality)
num_modality_animation <- num_modality_sum %>% filter(stimuli_modality == "animation") %>% pull(n)
num_modality_video <- num_modality_sum %>%  filter(stimuli_modality == "video") %>%  pull(n)

num_video_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "person") %>% count()
num_video_non_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "non_person") %>% count()

num_video_simultaneous <- ma_data %>% filter(presentation_type == "simultaneous") %>% count()
num_video_immediate_after <- ma_data %>% filter(presentation_type == "immediate_after") %>% count()
num_video_asynchronous <- ma_data %>% filter(presentation_type == "asynchronous") %>% count()
```

For methodologically relevant moderators, we coded the types of media used (video or animation) and the types of protagonists in the events (person or non-person). `r num_modality_video` conditions used video recordings of human actors (N = `r num_video_person`) or human actors in animal suits (N = `r num_video_non_person`). The other `r num_modality_animation` conditions used clips of animation with non-person figures as the protagonists of the events. We also coded how the onset of the linguistic stimuli aligned with the visual stimuli. The procedure was coded as "simultaneous" if the very first training sentence was presented along with the visual stimuli depicting relevant action (N = `r num_video_simultaneous`). It was coded as "immediately after" if it was presented along with an attention-getter or a blank screen, immediately followed by the relevant action (N = `r num_video_immediate_after`). Finally, some experimental conditions first presented the linguistic stimuli paired with irrelevant visual scenes (e.g. a person on the phone talking). The relevant visual stimuli were not shown until the training phase is over. For experimental conditions using this alignment, they were coded as "asynchronous"(N = `r num_video_asynchronous`).

```{r categorize_by_procedure}
num_point <- ma_data %>% filter(test_method == "point") %>% count()
num_look <- ma_data %>% filter(test_method == "look") %>% count()

num_practice_yes <- ma_data %>% filter(practice_phase == "yes") %>% count()
num_practice_no <- ma_data %>% filter(practice_phase == "no") %>% count()
num_char_id_yes <- ma_data %>% filter(character_identification == "yes") %>% count()
num_char_id_no <- ma_data %>% filter(character_identification == "no") %>% count()

num_mass <- ma_data %>% filter(test_mass_or_distributed == "mass") %>% count()
num_distributed <- ma_data %>% filter(test_mass_or_distributed == "distributed") %>% count()
```

We also coded aspects of experimental procedures. First, the type of responses elicited from the participants: whether the infants were explicitly prompted to point or their eye gaze duration were measured as they heard the linguistic stimuli (Pointing: N = `r num_point`; Looking: N = `r num_look`). Second, two characteristics of the experimental procedures were coded as categorical variables: the inclusion of practice phase (Yes: N = `r num_practice_yes` ; No: N = `r num_practice_no`), the inclusion of character-identification phase (Yes: N = `r num_char_id_yes`; No: N = `r num_char_id_no`). Third, the distribution of the training and the testing trials (Mass: N = `r num_mass`; Distributed: N = `r num_distributed`). A procedure was categorized as "mass" if and only if the infants were trained exclusively on one novel verb and tested on the very same verb. It was "distributed" if the infants were given multiple train and test pairs on multiple novel verbs. To better characterize the experience infants had prior to testing, we also coded how many train-test pair the infants were given (for mass procedure it was always 1), how many trials during the test phase infants were given, how many times the visual stimuli showing the relevant actions were presented, and how many times each novel verb was spoken in a syntactically-informative way. 
  
## Data Preprocessing 

```{r data_preprocessing}
num_has_raw <- ma_data %>% filter(!is.na(x_2_raw)) %>% distinct(unique_id) %>%  count()
```

We calculated the Cohen's d effect size for each experimental condition. Because we only included studies that used a two alternative forced-choice test method, we compared the mean proportion of correct response against chance level (0.5) for the group. Although some papers collected a baseline measure (N = `r num_has_raw`; e.g. [@arunachalam2013out; @arunachalam2019semantic; @naigles2011abstractness]), most papers did not. In order to calculate a consistent effect size measure across all condition, we used chance level as a baseline for calculating effect size for all conditions (see SI for comparision of effect sizes using reported baseline versus chance). 
  
  All the effect sizes and the coded variables were then analyzed with the metafor package in R [@viechtbauer2010].

# Results
```{r}
MODERATORS <- c( "NULL", "mean_age","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type","agent_argument_number", "n_repetitions_sentence", "n_repetitions_video", "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "visual_stimuli_pair", "test_method","presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )
mod_print <- generate_moderator_df(MODERATORS, ma_data)
```

## Publication Bias 

```{r label = "funnel", fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Funnel plots. Each point corresponds to an effect size estimate. The gray dashed line represnets the effect size of zero, the red dash lined represents the mean effect size, and the white funnel represents a 95% confidence interval around this mean."}
generate_funnel_plot(ma_data)
reg_model <- rma(ma_data$d_calc, ma_data$d_var_calc)
reg_result <- regtest(reg_model, model="rma", predictor="sei", ret.fit=FALSE) # access zval and pval from here
reg_z <- round(reg_result$zval, digits = 2)
reg_p <- if_else(reg_result$pval < 0.0001, "< .0001", as.character(reg_result$pval))
#reg_result
```

Publication bias refers to the tendency that journals would selectively publish on positive findings. This practice would inflate the real effect size and threaten the evidential value of the literature. In this section, we presented two analyses on publication bias, one using the traditional funnel plot approach combined with Egger’s regression test, the other using a newly established method known as publication bias sensitivity analysis [@mathur2020sensitivity]. We found evidence for publication bias, but the bias alone is not sufficient to explain the meta-analytic effect size.

 Upon first examination, the shape of the funnel plot is asymmetrical (Figure\ \@ref(fig:funnel)). We formally tested on the asymmetry by conducting Egger’s regression test using a multi-level model. The quantitative result confirms that the asymmetry in the funnel plot is statistically significant (*z* = `r reg_z`; *p* `r reg_p`). However, this does not necessarily imply publication bias. The classical publication bias analysis methods such as Egger’s regression test have assumptions that were often not met. First, Egger’s regression test assumes homogeneous effects. But the heterogeneity test on our dataset has significant results (*Q* = `r filter(mod_print, moderator == "NULL")$Q_print`; *p* `r filter(mod_print, moderator == "NULL")$Qp_print`), which violated this assumption. Second, the classical methods assume that the selection bias operates on the effect size. But in reality the p-value from statistics test was often the selection criterion. 

We addressed these concerns by supplementing our analysis with a sensitivity analysis method introduced by Marthur and VaderWeele (2020). Unlike its predecessors, this method did not have the homogeneity assumption nor the effect-size selection assumption. Instead, it estimated the severity of publication bias by considering how much more likely the “affirmative studies” will be published than the “non-affirmative studies”. Furthermore, it also calculated the severity of publication bias needed in the literature to “explain-away” the meta-analytic effect size. 

```{r label = "moderated_funnel", fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Significance funnel plot for Syntactic Bootstrapping literature. "}

significance_funnel(
ma_data$d_calc,
ma_data$d_var_calc,
xmin = min(ma_data$d_calc),
xmax = max(ma_data$d_calc),
ymin = 0,
ymax = max(sqrt(ma_data$d_var_calc)),
xlab = "Point estimate",
ylab = "Estimated standard error",
favor.positive = TRUE,
est.all = NA,
est.N = NA,
alpha.select = 0.05,
plot.pooled = TRUE
)

```

We conducted the sensitivity analysis by using the PublicationBias provided by Marthur and VaderWeele (2020). We found that [NOT QUITE SURE HOW TO INTERPRET THE ETA STUFF]. Moreover, the analysis also suggested that it is impossible to attenuate the meta-analytic effect size to zero by hypothetically increasing the severity of publication bias (REPORT SOME TEST?). This result can also be seen in the modified funnel plot (Figure\ \@ref(fig:moderated_funnel)). In this modified funnel plot, the x-axis represents the point estimate of Cohen’s d. The y-axis represents the estimated standard error. The gray line represents studies with a p-value of 0.05. To the left of the line are the non-affirmative studies (represented by gray dots), and to the right are the affirmative studies (represented orange dots). The diamonds near the x-axis are meta-analytic effect sizes. Note that the black one is pooling from all studies, whereas the gray one is only pooling from the non-affirmative studies. While the gray diamond is closer to the significance line than the black diamond, it does not cross the gray line. In other words, even in the scenarios with the worst publication bias, where we only consider the non-affirmative studies results, the meta-analytic effect size is still above zero. 

In conclusion, we found moderate evidence for publication bias, but the bias alone is not sufficient to explain away the observed effect. Some of the observed bias using the classical methods might be attributed to the heterogeneity in the data. In the following sections, we analyzed theoretical factors and methodological factors that were contributing to the heterogeneity. 



## Preliminary Analysis 

```{r label = "forest", fig.pos = "!t", fig.cap = "wow" , fig.height = 22, fig.width = 20 }
generate_forest_plot(ma_data)

```

The final analysis includent `r n_effect_sizes` independent effect sizes from `r n_papers` papers and contained testing results from `r unique_infants` unique infants. The weighted mean effect size in Cohen's d was `r filter(mod_print, moderator == "NULL")$estimate_print`, which was significantly different from 0  (*z* = `r filter(mod_print, moderator == "NULL")$z_print`; *p*: `r filter(mod_print,  moderator == "NULL")$p_print`). The range of effects sizes across different studies was shown in the forest plot (Figure\ \@ref(fig:forest)). 

## Theoretical Moderators
```{r}
age_vocab_cor <- cor.test(ma_data$mean_age,ma_data$productive_vocab_median)
cor_p <- ifelse(round(age_vocab_cor$p.value, digits = 2)<0.0001, "< 0.0001", as.character(round(age_vocab_cor$p.value, digits = 2)))
```

### Role of experience
Children do not perform better in the syntactic bootstrapping task as they get more experience. Although age and median productive vocabulary size correlated with each other (*r*(`r age_vocab_cor$parameter`) = `r round(age_vocab_cor$estimate, digits = 2)`, *p*: `r cor_p`), neither one was a significant predictor of the effect size in the mixed-effect linear regression model (Age: $\beta$ = `r filter(mod_print, moderator == "mean_age")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "mean_age")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "mean_age")$mod_z_print`, *p* `r filter(mod_print, moderator == "mean_age")$mod_p_print` ; Vocabulary Size: $\beta$ = `r filter(mod_print, moderator == "productive_vocab_median")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_z_print`, *p* `r filter(mod_print, moderator == "productive_vocab_median")$mod_p_print`).  

### Role of sentence structure 

How would the verbs’ syntactic environment and the semantic environment influence the learning outcome? To answer this question, we considered two moderators: the sentence structure and the types of words in the agent argument. The moderator sentence structure has two levels: transitive sentence and intransitive sentence. We fit a mixed effect model with the sentence structure as the single predictor. Our model suggests that children tested in the transitive conditions tend to have larger effect size than those tested under the intransitive conditions ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "sentence_structure")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "sentence_structure")$mod_z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$mod_p_print`). 

### Role of word types 

Like sentence structure, the moderator agent argument structure also has two levels: pronoun and noun. Using a similar model, we found that the agent argument structure is not a significant predictor of the effect size ($\beta$ = `r filter(mod_print, moderator == "agent_argument_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "agent_argument_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "agent_argument_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "agent_argument_type")$mod_p_print`). 


### Theoretical Moderators Model 
```{r}
theoretical_mod <- rma.mv(d_calc ~ sentence_structure + agent_argument_type + mean_age + productive_vocab_median, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/x_1,
                     method = "REML",
                     data = ma_data) 

theoretical_mod_print <- generate_mega_model_df(theoretical_mod)

#theoretical_mod_print
#summary(theoretical_mod)

```
 Finally, we also fit a mixed-effect model that includes all the moderators discussed above. Becasue there is no a priori reason to believe that these factors would interact with each other, we did not include interaction terms. In this model, the intercept is no longer significant ($\beta$ = `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_estimate_print_full`, *SE* = `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_SE_print`, *z* = `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_z_print`, *p* `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_p_print`). This suggests that the moderators can explain away some observed learning effects. The results for other predictors are consistent with the single-moderator models. Sentence structure is a significant predictor ($\beta$ = `r filter(theoretical_mod_print, moderator_name == "sentence_structuretransitive")$mod_estimate_print_full`, *SE* = `r filter(theoretical_mod_print, moderator_name == "sentence_structuretransitive")$mod_SE_print`, *z* = `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_z_print`, *p* `r filter(theoretical_mod_print, moderator_name == "intrcpt")$mod_p_print`), whereas agent argument type ($\beta$ = `r filter(theoretical_mod_print, moderator_name == "agent_argument_typepronoun")$mod_estimate_print_full`, *SE* = `r filter(theoretical_mod_print, moderator_name == "agent_argument_typepronoun")$mod_SE_print`, *z* = `r filter(theoretical_mod_print, moderator_name == "agent_argument_typepronoun")$mod_z_print`, *p* `r filter(theoretical_mod_print, moderator_name == "mean_age")$mod_p_print`), mean age ($\beta$ = `r filter(theoretical_mod_print, moderator_name == "mean_age")$mod_estimate_print_full`, *SE* = `r filter(theoretical_mod_print, moderator_name == "mean_age")$mod_SE_print`, *z* = `r filter(theoretical_mod_print, moderator_name == "mean_age")$mod_z_print`, *p* `r filter(theoretical_mod_print, moderator_name == "mean_age")$mod_p_print`), and median productive vocabulary size is not ($\beta$ = `r filter(theoretical_mod_print, moderator_name == "productive_vocab_median")$mod_estimate_print_full`, *SE* = `r filter(theoretical_mod_print, moderator_name == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(theoretical_mod_print, moderator_name == "productive_vocab_median")$mod_z_print`, *p* `r filter(theoretical_mod_print, moderator_name == "productive_vocab_median")$mod_p_print`). In summary, we found that sentence structure is a sigfnicant predictor of the effect size. Children tested with the transitive sentences performed better than those tested with intransitive sentences. The semantic content of the words in agent argument, that is, whether the words are nouns or pronouns, was not found to be a significant predictor. Neither age nor the median productive vocabulary size as measured by CDI was a significant predictor of the effect size.



## Methodological Moderators 

Motivated by the original @naigles1990children study, a majority of the research in Syntactic Bootstrapping drew heavily on the Intermodal Preferential Looking Paradigm @golinkoff1987eyes. Each of these later study designs often involved some study-specific adaptations of the paradigms. In this section, we consider how the various methodological changes may or may not have moderated the infants’ learning outcome. 

### Role of testing phases

Some studies added additional phases such as character identification phase or practice phase to help infants get used to the testing set-up. We found that neither the presence of the character identification phase ($\beta$ = `r filter(mod_print, moderator == "character_identification")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "character_identification")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "character_identification")$mod_z_print`, *p* `r filter(mod_print, moderator == "character_identification")$mod_p_print`) nor the practice phase ($\beta$ = `r filter(mod_print, moderator == "practice_phase")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "practice_phase")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "practice_phase")$mod_z_print`, *p* `r filter(mod_print, moderator == "practice_phase")$mod_p_print`) predicts the effect size. 

### Role of synchronicity.

Studies fall under three categories under the synchronicity between linguistic stimuli and visual stimuli: asynchronous, immediate-after and simultaneous. Asynchronous studies present the linguistic stimuli before the relevant visual stimuli. Immediate-after studies show the linguistc stimuli while the infants are exposed to attention-getter or blank-screen, and the visual stimuli were to follow immediately after. Simultaneous studies present the visual stimuli and the linguistic stimuli together. Our model result suggested that infants' learning outcomes were not influenced by the synchronicity between the stimuli. ($\beta$ = `r filter(mod_print, moderator == "presentation_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "presentation_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "presentation_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "presentation_type")$mod_p_print`). 

### Role of the testing procedure structure 
Last but not least, the structure of the testing procedure, mass testing or distributed testing, did not predict the effect size in a statistically meaningful way ($\beta$ = `r filter(mod_print, moderator == "test_mass_or_distributed")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "test_mass_or_distributed")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "test_mass_or_distributed")$mod_z_print`, *p* `r filter(mod_print, moderator == "test_mass_or_distributed")$mod_p_print`). Infants perfomed similarly in studies that have multiple train-test pair and single train-test pair. Furthermore, contrary to common beliefs, the number of exposures per novel verb the infants received prior to testing does not predict the effect sizes either ($\beta$ = `r filter(mod_print, moderator == "n_repetitions_sentence")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "n_repetitions_sentence")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "n_repetitions_sentence")$mod_z_print`, *p* `r filter(mod_print, moderator == "n_repetitions_sentence")$mod_p_print`).

### Methodological Moderators Model 
```{r message=FALSE, warning=FALSE}
methodological_mod <- rma.mv(d_calc ~ character_identification + practice_phase + presentation_type + test_mass_or_distributed + n_repetitions_sentence,
                             V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/x_1,
                     method = "REML",
                     data = ma_data)



methodological_mod_print <- generate_mega_model_df(methodological_mod)

#methodological_mod_print
#summary(methodological_mod)
```
We fit a model that included all the theoretical moderators considered above: the presence of character identification phase, the presence of practice phase, the synchronicity between linguistic stimuli and visual stimuli, the structure of testing procedure and the number of repetitions per novel verb prior to the test. Like the methodological moderators models, the intercept became insignificant ($\beta$ = `r filter(methodological_mod_print, moderator_name == "intrcpt")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "intrcpt")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "intrcpt")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "intrcpt")$mod_p_print`). Most of the moderators were still insignificant, with the exception of testing procedure structure. This moderator became marginally significant in the mega-model, suggesting that children tested in procedure with only one train-test pair performed better than those tested in procedure with multiple train-test pairs ($\beta$ = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_p_print`). The differences between predictors' performance in single-moderator model and mega-model can be seen in figure(???). 


# General Discussion

* Explanations for lack of age effect: variability in design as a function of age, change in input, etc. [Meta point: measuring developmental change requires constancy in task]
* compare to other effect sizes in language acquisition 
* Transitivity effect is robust - implications for this? Cross-linguistic implications?


\newpage

# References


---
nocite: |
...
  
```{r appendix}
#render_appendix("appendix.Rmd")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
