---
title             : "A meta-analysis of the syntactic bootsrapping phenomenon in word learning"
shorttitle        : "Syntactic Boostrapping MA"

author:  
  - name          : "Anjie Cao"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "what is this"
    email         : ""
  - name          : "Molly Y. Lewis"
    affiliation   : "2"
    email         : "okie"
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Department of Psychology, Carnegie Mellon Unviversity"
    
author_note: |
  okie

abstract: |
  so abstract!
  
keywords          : "keyword1"
wordcount         : ""

bibliography      : ["references.bib"]
header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}
  - \usepackage{hyphsubst}
  - \floatplacement{figure}{t!} # make every figure with caption = t

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
#lang              : "english"
documentclass     : "apa6"
class             : "man"
output:
  papaja::apa6_pdf:  #word_document 
    latex_engine: xelatex # this solved an encoding issue
    includes: 
      after_body: appendix.tex
---

```{r load_utility_packages, include = FALSE}
library(papaja)
library(rmarkdown)
library(tidyverse) 
library(here)
library(glue)
library(metafor)
library(knitr)
library(gridExtra)
library(here)
library(heatmaply)
library(MuMIn)
library(glmulti)
library(PRISMAstatement)
#source("/Users/caoanjie/Desktop/Summer2020/SyntacticBootstrapping/exploratory_analyses/01_es_analyses/scripts/04_prisma_diagram.R") 
```


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = FALSE,
                      echo = FALSE,
                      fig.pos = "t!")
```


```{r read in data}
DATA_PATH <- here("data/processed/syntactic_bootstrapping_tidy_data.csv") 
RAW_DATA_PATH <- here("data/raw/syntactic_bootstrapping_raw_data.csv")

ma_data <- read_csv(DATA_PATH)   
```

```{r }

# prisma(found = records_identified_google_scholar + records_identified_forward_search,
#        found_other = 123,
#        no_dupes = 776, 
#        screened = 776, 
#        screen_exclusions = 13, 
#        full_text = 763,
#        full_text_exclusions = 17, 
#        qualitative = 746, 
#        quantitative = 319)


# my_prisma_plot <- prisma2(found = records_identified_google_scholar + records_identified_forward_search, # how many unique papers
#         found_other = records_identified_forward_search,  # how many papers did you find through other sources?
#         screened = total_records_screened, # how many of those papers did you screen by looking at the title/abstract?
#         screen_exclusions = screen_exclusions, # how many of those papers that you screened did you exclude?
#         full_text = full_text_assesed_for_eligibility, # how many papers did you look at the full text for?
#         full_text_exclusions = full_text_assesed_exclusion, # how many papers did you exclude after looking at the ful text?
#         quantitative = final_inclusion, # how many papers went in your final meta-analysis
#         width = 800, height = 800)
# 
# my_prisma_plot
```

# Introductions 
Children build up their vocabularies at a dazzling speed.Toward the end of the second year, they were estimated to have a productive vocabulary size of around 300 [@fenson1994variability].But children’s impressive word-learning abilities do not apply equally to words of all kinds. Verbs, for example, constitute a special challenge for English speaking children. Numerous observational studies have shown that verbs are often learned later than nouns [@nelson1973structure; @goldin1976language; @lieven1992individual; @longobardi2017early; @nelson1993nouns;@bornstein2004cross], and laboratory experiments also found that children’s verb learning is more slowly and difficult than noun learning[@gentner1978relational;@childers200612;@schwartz1984words; @oviatt1980emerging;@childers2012children;@imai2008novel;@abbot2017role;@childers2002two]. The challenge of verb learning is often attributed to the underlying representational differences between verbs and nouns: Unlike nouns, the references of verbs are ephemeral and ever-changing. As a consequence, in order to learn verbs, children need to rely less on the unreliable extralinguistic information, and rely more on the information within the verbs’ linguistic context [@gentner2001individuation;@gentner2006verbs]. 

One hypothesized verb-learning mechanism is known as “Syntactic Bootstrapping”: children can use the syntactic information in the sentence to narrow down the verbs’ meaning [@Landau1985; @gleitman1990structural]. The idea that syntactic information can shape children’s interpretations of novel words was not new. As early as the late 1950s, a classic study by @brown1957linguistic has shown that 3- to 5-year-old children would choose to map a novel word to an action if the word is introduced in a verb context (e.g.“Do you know what it means to sib?”), and map to an object if introduced in a noun context(e.g.“Do you know what a sib is?”). However, this early finding did not specify how detailed the childrens’ understanding of the verb meanings is. And since the children were relatively older, it was unclear whether the syntax could help word learning at an earlier age. Later, the introduction of Intermodal Preferential Looking Paradigm made it possible to test word learning in younger children [@golinkoff1987eyes]. A seminal paper by @naigles1990children first put the Syntactic Bootstrapping hypothesis into test on children. She found that as young as 25-month-old, children's understanding of novel verbs are informed by the syntactic structure in a suprisingly detailed way, in that they can map a novel verb to a causal action between two agents or a synchronous non-causal action consisting of two agents, depending whether the novel verb was embedded in a transitive structure or an intransitive structure. 


# Methods

## Literature Search  

```{r literature search, search procedure}
records_identified_google_scholar <- 2330
records_identified_forward_search <- 1009
records_identified_review_reference <- 155
records_identified_experts_in_the_field <- 11
total_records_identified <- records_identified_google_scholar + records_identified_forward_search + records_identified_review_reference + records_identified_experts_in_the_field

records_screened_google_scholar <- 10 * 60 
records_screened_forward_search <- 10 * 10 
records_screened_review_reference <- 155 
records_screened_experts_in_the_field <- 14

# Yikes this code was hard to read! Line breaks are your friend! :) Minimally put a line break after each pipe. 
total_records_screened <- records_screened_google_scholar +
  records_screened_forward_search + 
  records_screened_review_reference + 
  records_screened_experts_in_the_field

# class project + else
full_text_assesed_for_eligibility <- 503 + 166
final_inclusion <- read_csv(RAW_DATA_PATH) %>% 
  filter(paper_eligibility == "include") %>%
  distinct(unique_id) %>%
  count()


screen_exclusions <- total_records_screened - full_text_assesed_for_eligibility
full_text_assesed_exclusion <- full_text_assesed_for_eligibility -
  final_inclusion
```


<!-- you can italicize parameterize like "N" with "$N$" --> 
We conducted our literature search following the  Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist [PRISMA; @moher2009]. We identified relevant papers by conducting a keyword search in Google Scholar with the phrase "Syntactic Bootstrapping" and a forward search on papers that cited the seminal paper [@naigles1990children] (total records identified: $N$ =  `r records_identified_google_scholar + records_identified_forward_search`; retrieved between May 2020 and July 2020). We screened the first 60 pages (N = 600) of the keyword search results and the first 10 pages (N = 100) of the forward search results. The screening processes ended because we could no longer identify relevant, non-duplicate papers from consecutive pages. Additional papers were identified by consulting the references section of the most recent literature review (N = `r records_identified_review_reference`) [@fisher2020developmental] and the experts in the field (N = `r records_identified_experts_in_the_field`). In our final sample, we included published journal articles, conference proceedings, doctoral dissertations, and unpublished manuscripts. They will be collectively referred to as "papers" in the following sections. Each paper may include multiple experimental conditions, and thus provides multiple effect sizes for the final analysis.

We restricted our final sample to papers that satisfied the following conditions: First, the experimental paradigm involved a two-alternative forced-choice task, in which the participants were instructed to identify the scene that matched the linguistic stimuli. Second, the visual stimuli were displayed on a monitor. We included videos that were either recordings of actors or animated clips. Third, the linguistic stimuli included at least one novel verb embedded in a syntactically informative frame. For example, "Look, it's kradding!" embeds the novel verb in an intransitive syntactic frame that is informative about the meaning of the novel verb "kradding", while "Look, Kradding!" does not and therefore would be excluded. Finally, we resricted our sample to English-speaking, typically-developing children.
  
```{r literature search - report final sample}
n_effect_sizes <- ma_data %>%
  filter(!is.na(d_calc)) %>%
  nrow()

n_papers <- ma_data %>%
  distinct(unique_id) %>%
  nrow()

unique_infants <- ma_data %>% 
  filter(unique_infant == "unique_condition") %>% 
  filter(adult_participant == "no") %>% 
  summarize(sum_infants = sum(n_1))

non_unique_infants <- ma_data %>% 
  filter(unique_infant == "not_unique") %>% 
  filter(adult_participant == "no") %>%
  distinct(short_cite, expt_condition, .keep_all = TRUE) %>% 
  summarize(sum_infants = sum(n_1))

different_infants <- unique_infants + non_unique_infants
mean_age_day <- ma_data %>% filter(!is.na(mean_age)) %>% summarize(mean_age =  format(round(mean(mean_age), 2)))

```
  
<!-- There's a bunch of missing values in the pdf, not sure why--> 
   Our final sample included data from `r different_infants` unique infants (Mean age: `r mean_age_day` Days), reported in `r n_effect_sizes` individual effect sizes from `r n_papers` individual papers. 
   
## Data Entry

For each paper, we entered the paper metadata (e.g., citation), information to calculate effect sizes, and moderators. We entered a separate effect size for each experimental condition and age group in paper. Most papers therefore contained multiple effect sizes.

```{r data_source_summary}
data_source_summary <- ma_data %>% group_by(data_source_clean) %>% count()
num_author_contact <- data_source_summary %>% filter(data_source_clean == "author_contact") %>% pull(n)
num_plot <- data_source_summary %>% filter(data_source_clean == "plot") %>% pull(n)
num_table <- data_source_summary %>% filter(data_source_clean == "table") %>% pull(n)
num_text <- data_source_summary %>% filter(data_source_clean == "text") %>% pull(n)

```

In order to calculate the effect sizes, we recorded the sample size of each condition, the group mean, and the across-participant standard deviation. The mean and standard deviation were obtained from one of the three ways: a) directly retrieved from the results section or the data-presenting tables (N = `r num_table + num_text`); b) recovered from the plots by measuring the height of the bars and the error bars (N =`r num_plot`); c) contacted the original authors (N = `r num_author_contact`). For looking time studies, when the paper only reported the raw looking time in seconds, we calculated the proportion of correct response by dividing the mean looking time toward the matching scenes by the sum of mean looking time toward the matching scenes and the mean looking time toward the non-matching scenes (i.e., excluding the look away time from the denominator). The standard deviations were also scaled by being divided by the sum. <!-- what do you mean by scaled here?-->


```{r categorize_by_proto}
num_proto_approach <- ma_data %>% filter (inclusion_certainty == 2) %>% count()
num_atypical_approach <- ma_data %>% filter (inclusion_certainty == 1) %>% count()
```

We classified our final samples of effect sizes (N = `r n_effect_sizes`) into two categories: the prototypical ones (N = `r num_proto_approach`) and atypical ones (N = `r num_atypical_approach`). Prototypical studies were the ones in which the set-up and the procedures were most similar to the seminal paper [@naigles1990children]. The atypical studies were experiments that met all of the inclusion criteria, but deviated from the prototypical designs in some non-trivial ways (The stimuli involved more complex syntactic structure or event structure: N = `r n_effect_sizes - 12`; The testing procedure included a contrast phase: N = 10; The experiment was conducted through online platform: N = 2). To preserve the homogeneity of the experiments without reducing the power of our analysis, we conducted our statistical analysis both on the entire sample and specifically on the prototypical studies.

```{r vocb_available}
num_vocabulary_available <- ma_data %>% filter(!is.na(productive_vocab_median)) %>% count()
```

For all experimental conditions, we coded three types of moderators: participant moderators,  stimuli moderators, and  procedure moderators. The information was either retrieved from the methods section of the paper or obtained via contacting authors. 

At the participant level, we coded infants' mean age in days (N= `r n_effect_sizes`) and the median productive vocabulary measured by MacArthur-Bates Communicative Development Inventories (CDI) Words and Sentences <!--add citation--> (`r  num_vocabulary_available`). 

```{r categorize_by_linguistic_stimuli}
num_transitive <- ma_data %>% filter(sentence_structure == "transitive") %>% count()
num_intransitive <- ma_data %>% filter(sentence_structure == "intransitive") %>% count()

num_agent_sum <- ma_data %>% count(agent_argument_type_clean)
num_agent_noun <- num_agent_sum %>% filter(agent_argument_type_clean == "noun") %>% pull(n)
num_agent_noun_phrase <- num_agent_sum %>% filter(agent_argument_type_clean == "noun_phrase") %>% pull(n) 
num_agent_pronoun <- num_agent_sum %>% filter(agent_argument_type_clean == "noun_phrase") %>% pull(n)
num_agent_two_noun <- num_agent_sum %>% filter(agent_argument_type_clean == "two_noun") %>% pull(n)
num_agent_varying <- num_agent_sum %>% filter(agent_argument_type_clean == "varying_agent") %>% pull(n)

num_patient_sum <-  ma_data %>% count(patient_argument_type_clean)
num_patient_noun <- num_patient_sum %>% filter(patient_argument_type_clean == "noun") %>% pull(n)
num_patient_noun_phrase <- num_patient_sum %>% filter(patient_argument_type_clean == "noun_phrase") %>% pull(n)
num_patient_pronoun <- num_patient_sum %>% filter(patient_argument_type_clean == "pronoun") %>% pull(n)
num_patient_varying <- num_patient_sum %>% filter(patient_argument_type_clean == "varying_patient") %>% pull(n)

num_test_agent <- ma_data %>% filter(test_type == "agent") %>% count()
num_test_action <- ma_data %>% filter(test_type == "action") %>% count()

```

We coded features of the linguistic stimuli during both training and testing. The training linguistic stimuli were categorized as transitive (N = `r num_intransitive`), if the novel verbs were embedded in a sentence with two or more noun arguments, and  intransitive (N = `r num_intransitive`) if the novel verbs were embedded in a sentence with one noun argument. We also coded the types of words used in the agent argument (One noun: N = `r num_agent_noun`; One pronoun: N = `r num_agent_pronoun`; Two nouns: N = `r num_agent_two_noun`; Noun phrase: N = `r num_agent_noun_phrase`; Varying across sentences: N = `r num_agent_varying`) and the patient argument(One noun: N = `r num_patient_noun`; One pronoun: N = `r num_patient_pronoun`; Noun phrase: N = `r num_patient_noun_phrase`; Varying across sentences: N = `r num_patient_varying`). We coded the  testing linguistic stimuli for whether infants were prompted to identify the action (e.g. "Where's lorping? Find lorping!", N = `r num_test_agent`) or the actors (e.g. "Which one (verbed) the other one....point!", N = `r num_test_action`). 

```{r categorize_by_visual_stimuli}
num_modality_sum <- ma_data %>% count(stimuli_modality)
num_modality_animation <- num_modality_sum %>% filter(stimuli_modality == "animation") %>% pull(n)
num_modality_video <- num_modality_sum %>%  filter(stimuli_modality == "video") %>%  pull(n)

num_video_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "person") %>% count()
num_video_non_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "non_person") %>% count()

num_video_simultaneous <- ma_data %>% filter(presentation_type == "simultaneous") %>% count()
num_video_immediate_after <- ma_data %>% filter(presentation_type == "immediate_after") %>% count()
num_video_asynchronous <- ma_data %>% filter(presentation_type == "asynchronous") %>% count()


```

For visual stimuli, we coded the types of media used (video or animation) and the types of protagonists in the events (person or non-person). `r num_modality_video` conditions used video recordings of human actors (N = `r num_video_person`) or human actors in animal suits (N = `r num_video_non_person`). The other `r num_modality_animation` conditions used clips of animation with non-person figures as the protagonists of the events. We also coded how the onset of the linguistic stimuli aligned with the visual stimuli. The procedure was coded as "simultaneous" if the very first training sentence was presented along with the visual stimuli depicting relevant action (N = `r num_video_simultaneous`). It was coded as "immediately after" if it was presented along with an attention-getter or a blank screen, immediately followed by the relevant action (N = `r num_video_immediate_after`). Finally, some experimental conditions first presented the linguistic stimuli paired with irrelevant visual scenes (e.g. a person on the phone talking). The relevant visual stimuli were not shown until the training phase is over. For experimental conditions using this alignment, they were coded as "asynchronous"(N = `r num_video_asynchronous`).

```{r categorize_by_procedure}
num_point <- ma_data %>% filter(test_method == "point") %>% count()
num_look <- ma_data %>% filter(test_method == "look") %>% count()

num_practice_yes <- ma_data %>% filter(practice_phase == "yes") %>% count()
num_practice_no <- ma_data %>% filter(practice_phase == "no") %>% count()
num_char_id_yes <- ma_data %>% filter(character_identification == "yes") %>% count()
num_char_id_no <- ma_data %>% filter(character_identification == "no") %>% count()

num_mass <- ma_data %>% filter(test_mass_or_distributed == "mass") %>% count()
num_distributed <- ma_data %>% filter(test_mass_or_distributed == "distributed") %>% count()
```

Finally, procedure moderators included the types of response elicited from the participants: whether the infants were explicitly prompted to point or their eye gaze duration were measured as they heard the linguistic stimuli (Pointing: N = `r num_point`; Looking: N = `r num_look`). Three characteristics of the experimental procedures were coded as categorical variables: the inclusion of practice phase (Yes: N = `r num_practice_yes` ; No: N = `r num_practice_no`), the inclusion of character-identification phase (Yes: N = `r num_char_id_yes`; No: N = `r num_char_id_no`), and the distribution of the training and the testing trials (Mass: N = `r num_mass`; Distributed: N = `r num_distributed`). A procedure was categorized as "mass" if and only if the infants were trained exclusively on one novel verb and tested on the very same verb. It was "distributed" if the infants were given multiple train and test pairs on multiple novel verbs. To better characterize the experience infants had prior to testing, we also coded how many train-test pair the infants were given (for mass procedure it was always 1), how many trials during the test phase infants were given, how many times the visual stimuli showing the relevant actions were presented, and how many times each novel verb was spoken in a syntactically-informative way. 
  
## Data Preprocessing 

```{r data_preprocessing}
num_has_raw <- ma_data %>% filter(!is.na(x_2_raw)) %>% count()
```

  We calculated the Cohen's d effect size for each experimental condition. Because we only included studies that used a two alternative forced-choice test method, we compared the mean proportion of correct response against chance level (0.5) for the group. Although some papers collected a baseline measure (N = `r num_has_raw`; e.g. XXX, XXX), most papers did not. In order to calculate a consistent effect size measure across all condition, we used chance level as a baseline for calculating effect size for all conditions (see SI for comparision of effect sizes using reported baseline versus chance). 
  
  All the effect sizes and the coded variables were then analyzed with the metafor package in R [@viechtbauer2010].

# Results

```{r child = "sections/section1.Rmd"}
```

# General Discussion


\newpage

# References

---
nocite: |
...
  
```{r appendix}
render_appendix("appendix.Rmd")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
