---
title             : "A meta-analysis of the syntactic bootsrapping phenomenon in word learning"
shorttitle        : "Syntactic Boostrapping MA"

author:  
  - name          : "Anjie Cao"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "what is this"
    email         : ""
  - name          : "Molly Y. Lewis"
    affiliation   : "2"
    email         : "okie"
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Department of Psychology, Carnegie Mellon Unviversity"
    
author_note: |
  okie

abstract: |
  so abstract!
  
keywords          : "keyword1"
wordcount         : ""

bibliography      : ["references.bib"]
csl               : "apa6-meta.csl"

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}
  - \usepackage{hyphsubst}
  - \floatplacement{figure}{t!} # make every figure with caption = t

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
#lang              : "english"
documentclass     : "apa6"
class             : "man"
output:
  papaja::apa6_pdf:  #word_document 
    latex_engine: xelatex # this solved an encoding issue
    includes: 
      after_body: appendix.tex
---

```{r load_utility_packages, include = FALSE}
library(papaja)
library(rmarkdown)
library(tidyverse) 
library(here)
library(glue)
library(metafor)
library(knitr)
library(gridExtra)
library(here)
library(heatmaply)
library(MuMIn)
library(glmulti)
library(PRISMAstatement)
#source("/Users/caoanjie/Desktop/Summer2020/SyntacticBootstrapping/exploratory_analyses/01_es_analyses/scripts/04_prisma_diagram.R") 
```


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = FALSE,
                      echo = FALSE,
                      fig.pos = "t!")
```


```{r read in data}
DATA_PATH <- here("data/processed/syntactic_bootstrapping_tidy_data.csv") 
RAW_DATA_PATH <- here("data/raw/syntactic_bootstrapping_raw_data.csv")

ma_data <- read_csv(DATA_PATH)   
```

```{r }

# prisma(found = records_identified_google_scholar + records_identified_forward_search,
#        found_other = 123,
#        no_dupes = 776, 
#        screened = 776, 
#        screen_exclusions = 13, 
#        full_text = 763,
#        full_text_exclusions = 17, 
#        qualitative = 746, 
#        quantitative = 319)


# my_prisma_plot <- prisma2(found = records_identified_google_scholar + records_identified_forward_search, # how many unique papers
#         found_other = records_identified_forward_search,  # how many papers did you find through other sources?
#         screened = total_records_screened, # how many of those papers did you screen by looking at the title/abstract?
#         screen_exclusions = screen_exclusions, # how many of those papers that you screened did you exclude?
#         full_text = full_text_assesed_for_eligibility, # how many papers did you look at the full text for?
#         full_text_exclusions = full_text_assesed_exclusion, # how many papers did you exclude after looking at the ful text?
#         quantitative = final_inclusion, # how many papers went in your final meta-analysis
#         width = 800, height = 800)
# 
# my_prisma_plot
```

# Introductions 
Children build up their vocabularies at a dazzling speed. Toward the end of the second year, they were estimated to have a productive vocabulary size of around 300 [@fenson1994variability]. But children’s impressive word-learning abilities do not apply equally to words of all kinds. Verbs, for example, constitute a special challenge for children. Numerous observational studies have shown that verbs are often learned later than nouns [@nelson1973structure; @goldin1976language; @lieven1992individual; @longobardi2017early; @nelson1993nouns], and laboratory experiments also found that children’s verb learning is more slowly and difficult than noun learning[@gentner1978relational;@childers200612;@schwartz1984words; @oviatt1980emerging;@childers2012children;@imai2008novel;@abbot2017role;@childers2002two].  Some scholars have argued that the challenge of verbs is language-dependent [@choi1995early; @tardif1996nouns; @tardif1999putting]. Nevertheless, the cross-linguistic findings are mixed, with others found consistent patterns across languages [@au1994input; @papaeliou2011vocabulary; @bornstein2004cross;@kim2000early].  A more recent large scale, cross-linguistic corpus analysis has shown that across 23 different languages, verbs and other predicates, when compared with nouns, constitute a smaller proportion of children’s early production vocabularies, though there are more cross-linguistic differences in early comprehension vocabularies [@frank2019variability]. Together, these findings suggest that at least certain aspects of the challenge associated with verb learning are universal across languages. Some have hypothesized that it is due to the nature of verbs’ references: Unlike nouns, the references of verbs are ephemeral and ever-changing. As a consequence, to learn verbs, children need to rely less on the unreliable extralinguistic information and rely more on the information within the verbs’ linguistic context, such as syntax[@gentner2001individuation, @gentner2006verbs].   

The idea that syntactic information can shape children’s interpretations of novel words was not new. As early as the late 1950s, a classic study by @brown1957linguistic has shown that 3- to 5-year-old children would choose to map a novel word to an action if the word is introduced in a verb context (e.g.“Do you know what it means to sib?”), and map to an object if introduced in a noun context(e.g.“Do you know what a sib is?”). However, this early finding only provided evidence for older children’s abilities to discriminate between syntactic categories. It remained unknown how early the abilities emerge and how detailed the underlying representations for the verbs are. In the late 1980s and early 1990s, a hypothesis known as “Syntactic Bootstrapping” revived the interests in the early interaction between syntax and verbs’ semantics during language development [@gleitman1990structural; @Landau1985]. The basic idea of Syntactic Bootstrapping is that the structures of the sentences were potent cues for children to interpret the meanings of the verbs. This idea soon received empirical support from a seminal study by @naigles1990children. By using the Intermodal Preferential Looking Paradigm [@golinkoff1987eyes], @naigles1990children provided the first evidence for 25-month-olds children’s abilities to incorporate syntactic cues in verbs learning. 

In this seminal study, the children were tested on four different novel verbs. For each novel verb, there was a training phase and a testing phase. During the training phase, the children would hear either a series of transitive sentences or intransitive sentences, depending on the conditions they were assigned to. Each sentence grammatically contained the novel verb (e.g., for transitive: ”The duck is gorping the bunny”; for intransitive: “The duck and the bunny are gorping”.) The children were also exposed to the visual stimuli depicting a two-actor action while listening to these sentences. The two actors, one in a bunny suit and the other in a duck suit, would perform two novel actions simultaneously. They would move their arms synchronously while the one pushed the other bending forward. After the training phase, the children would be prompted to look for the action (e.g. “Where’s the gorping now?”). The children faced two screens side-by-side, one would show the bunny and duck performing non-causal synchronous arm movements, while the other would show the causal pushing-and-bending action. The children’s fixation time toward each of the screens was recorded. A longer looking time for either of the screen indicated a match between the children’s conceptual representations of the novel verbs and the action on the screens. @naigles1990children found that children assigned to the transitive condition looked longer at the causal action than the non-causal action, whereas the children in the intransitive condition showed the opposite pattern. This indicated that at this age children can interpret the meanings of novel verbs as informed by the syntactic structures in a surprisingly detailed way.

This paradigm soon became a standard in studying Syntactic Bootstrapping in young children. Numerous experiments have adopted and extended this paradigm to explore different facets of Syntactic Bootstrapping. Many variations of this paradigm were developed: using pointing instead of looking as the behavioral responses measured [e.g., @arunachalam2010meaning; @kline2017linking; @rowland2010role]; using human actors as the protagonists of the actions instead of the humans in animal suits [e.g., @gertner2012predicted; @bunger2006constrained; @messenger2015learning]; adding a practice phase or a character-identification phase to reduce the task demands for young children [@scott2009two;@gertner2012predicted;@scott2017lookit]. The changes can potentially bring unintended effects on the learning outcomes: studies differ significantly in the amount of training experience provided to the participants. Some studies, similar to the original @naigles1990children design, distributed the training experience across multiple different novel verbs [e.g., @he2020two; @arunachalam2013two; @naigles1993first; @naigles1996use]. Others, by the contrary, focused on training and testing on one novel verb throughout the experiment [e.g., @jin2015role;  @kline2017linking;@messenger2015learning]. As a result, children may have heard the novel verb repeated from as little as 3 times [e.g. @naigles1990children; @naigles1993first; @naigles1996use], to as many as 27 times [e.g., @arunachalam2013two] before they were tested. There are also changes motivated by theoretical considerations. For instance, whether the syntactic cues alone were sufficient to support verbs learning has been studied by altering the visual stimuli during the training phase. Rather than seeing the visual stimuli representing the relevant actions, the children in some training phases would see two actors talking to each other or one actor talking over the phone [e.g. @yuan2009really]. The absence of extralinguistic cues did not prevent the children from interpreting the novel verbs correctly, which further suggests the significance of the syntactic cues in verb learning at an early age.  

The changes in paradigms have also led to some inconsistent findings. For example, The results have been mixed on whether the children have a transitivity bias, that is, children are more likely to match the correct scenes when the novel verbs are embedded in the transitive frames. Some experiments found that children who were exposed to the transitive sentences would show a larger preference to the matching scenes, whereas children exposed to the intransitive sentences looked by chance or showed a looking pattern similar to the control group who were not provided with any relevant syntactic cues [@yuan2012counting; @arunachalam2010meaning]. This transitivity advantage can be attributed to three factors: the clarity, the experience, and the perceptual saliency of the stimuli. First, under certain contexts, the transitive sentences have less ambiguity than their intransitive counterparts.  If one sees two scenes: in one a girl patting a boy, in the other a girl and a boy jumping side-by-side, then upon hearing “the girl is gorping the boy”, only the scene with patting action is a plausible interpretation. In contrast, if one hears the intransitive sentence, “the girl is gorping”,  the verb can be interpreted as being consistent with both the causative scene and the non-causative scene. Second, children may have more experience with transitive sentences than intransitive sentences. A corpus analysis on parental utterances for 1- to 6-year-old children found that transitive sentences make up approximately 24.36% of all utterances, whereas intransitive sentences occupy only 17.24% [@laakso2007pronouns]. Thus the familiarity with transitive sentences can potentially improve children’s understanding of transitive sentences in the lab. Last but not least, the perceptual saliency of the visual scenes may have contributed to the asymmetrical performance in the transitive conditions and the transitive conditions. Children were found to have a baseline preference for two actors event over one actor event [e.g., @yuan2012counting], and for synchronous movement over causative movement [e.g., @naigles1993first]. These preferences can make detecting an effect in the transitive condition easier than in the intransitive condition. 

However, other studies have found the opposite pattern. In the second and the third experiment in @naigles1993first, it was children in the transitive conditions who showed no preference for neither of the events, whereas children in the intransitive conditions looked longer at the matching event. Similarly, @bunger2004syntactic also showed that children exposed to the intransitive sentences tend to look longer at the matching scene but those exposed to the transitive sentences look by chance.  There were two proposed explanations for this reversed effect. First, transitive verbs can be more ambiguous under certain contexts due to children’s sensitivity to the internal structure of the event. Research has shown that children under 1 year of age already possess impressive abilities to detect the internal structure of an event. Following the structure, they can parse the event into subcomponents[@hespos2009infants; @hespos2010infants; @stahl2014infants; for a recent review, see @levine2019finding]. On the one hand, successful parsing underlies successful verb learning [@friend2011beyond]. But on the other hand, this sensitivity to the internal structure of the event can also lead to challenges for learning transitive verbs. As @bunger2004syntactic pointed out, the transitive verbs can denote both the means of the action and the results of the action. In comparison, the possible meanings of intransitive verbs are more constrained to the results of the action. The wider range of possible meanings could make it more difficult for the children to correctly match the sentences and the scenes. The second possibility for this asymmetry is the processing demands brought by the words in the argument positions. @lidz2009one noted that the minimal demand from a simple transitive sentence is for children to map at least two nouns, the subject and the object to the corresponding agent and the patient. But for an intransitive sentence, the demand can be as low as only mapping only the subject of the sentence to the agent of the scene. In consequence, learning the novel verbs in the intransitive sentences can be easier than in the transitive sentences. 

Besides the number of words in the verbs’ arguments, the semantic content is also considered to be a key moderator for the learning process. Some scholars have proposed that semantically rich contexts are beneficial for verb learning, because the semantic content of the surrounding nouns can scaffold the interpretation of the verbs, together with the syntactic structure [@arunachalam2011grammatical; @arunachalam2015let; @gillette1999human; @piccin2007nouns; @gleitman2005hard; @fisher1994better; @imai2008novel]. But the empirical support for this view is mixed. In a recent study, @he2020linguistic manipulated the amount of semantic content in the subject argument. Some preschoolers were provided with more information about the subject (“The tall girl is fezzing”), and others less (“The girl is fezzing”). Contrary to the semantic-scaffolding view, @he2020linguistic found that the children learned better when the nouns were not modified, indicating that the children’s limited information processing abilities may have impeded them from utilizing the semantic content. This pattern also persists when the number of words in the agent argument is controlled for. Pronouns have less semantic content than concrete nouns. Multiple studies have found that preschool children learn verbs better when the verbs co-occur with pronouns instead of nouns. They are not only more likely to correctly identify the scenes corresponding to the linguistic stimuli but also are more likely to generalize the syntactic frames to new verbs [@lidz2009one; @childers2001role].

To further shed light on these conflicting findings, we decided to conduct a meta-analysis on syntactic bootstrapping literature. Our first goal is to estimate the robustness of this phenomenon by calculating the meta-analytic effect size. Knowing the effect size is consequential to theory building [@lewis2016quantitative; @bergmann2018promoting]. Many past meta-analyses have tapped into other domains of early language development such as phonotactic learning [@cristia2018can], word segmentation [@bergmann2016development], and mutual exclusivity [@lewis2020role]. To our knowledge, this is the first meta-analysis examining the development of syntactic abilities in early childhood and its influence on word learning. Our second goal is to examine the potential moderators of the effect. Understanding the moderators can be valuable for both theoretical reasons and practical reasons. Due to the limited sample sizes, individual infant experiments often have insufficient statistical power, which can make detecting relevant moderators difficult [@oakes2017sample]. The variations among different testing procedures also made it difficult to reconcile the conflicting findings, because the differences in the observed effect can be caused by a combination of methodological factors, rather than reflecting the differences in the measured constructs. The meta-analytic approach has the opportunity to reveal some of the underlying interactions between different factors, which can inform future researchers about their experimental designs. Last but not least, our fourth goal is to evaluate how the effect changes with age. Most of the syntactic-bootstrapping studies used a cross-sectional design. Despite the virtue of being rigorously controlled, the “time-slice” nature of this design made it difficult to reconstruct a continuous developmental trajectory. The meta-analysis can provide unique insights into how children’s abilities to incorporate syntactic information into verbs learning to develop in the first three years of life. 



# Methods

## Literature Search  

```{r literature search, search procedure}
records_identified_google_scholar <- 2330
records_identified_forward_search <- 1009
records_identified_review_reference <- 155
records_identified_experts_in_the_field <- 11
total_records_identified <- records_identified_google_scholar + records_identified_forward_search + records_identified_review_reference + records_identified_experts_in_the_field

records_screened_google_scholar <- 10 * 60 
records_screened_forward_search <- 10 * 10 
records_screened_review_reference <- 155 
records_screened_experts_in_the_field <- 14

# Yikes this code was hard to read! Line breaks are your friend! :) Minimally put a line break after each pipe. 
total_records_screened <- records_screened_google_scholar +
  records_screened_forward_search + 
  records_screened_review_reference + 
  records_screened_experts_in_the_field

# class project + else
full_text_assesed_for_eligibility <- 503 + 166
final_inclusion <- read_csv(RAW_DATA_PATH) %>% 
  filter(paper_eligibility == "include") %>%
  distinct(unique_id) %>%
  count()


screen_exclusions <- total_records_screened - full_text_assesed_for_eligibility
full_text_assesed_exclusion <- full_text_assesed_for_eligibility -
  final_inclusion
```


<!-- you can italicize parameterize like "N" with "$N$" --> 
We conducted our literature search following the  Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist [PRISMA; @moher2009]. We identified relevant papers by conducting a keyword search in Google Scholar with the phrase "Syntactic Bootstrapping" and a forward search on papers that cited the seminal paper [@naigles1990children] (total records identified: $N$ =  `r records_identified_google_scholar + records_identified_forward_search`; retrieved between May 2020 and July 2020). We screened the first 60 pages (N = 600) of the keyword search results and the first 10 pages (N = 100) of the forward search results. The screening processes ended because we could no longer identify relevant, non-duplicate papers from consecutive pages. Additional papers were identified by consulting the references section of the most recent literature review (N = `r records_identified_review_reference`) [@fisher2020developmental] and the experts in the field (N = `r records_identified_experts_in_the_field`). In our final sample, we included published journal articles, conference proceedings, doctoral dissertations, and unpublished manuscripts. They will be collectively referred to as "papers" in the following sections. Each paper may include multiple experimental conditions, and thus provides multiple effect sizes for the final analysis.

We restricted our final sample to papers that satisfied the following conditions: First, the experimental paradigm involved a two-alternative forced-choice task, in which the participants were instructed to identify the scene that matched the linguistic stimuli. Second, the visual stimuli were displayed on a monitor. We included videos that were either recordings of actors or animated clips. Third, the linguistic stimuli included at least one novel verb embedded in a syntactically informative frame. For example, "Look, it's kradding!" embeds the novel verb in an intransitive syntactic frame that is informative about the meaning of the novel verb "kradding", while "Look, Kradding!" does not and therefore would be excluded. Finally, we resricted our sample to English-speaking, typically-developing children.
  
```{r literature search - report final sample}
n_effect_sizes <- ma_data %>%
  filter(!is.na(d_calc)) %>%
  nrow()

n_papers <- ma_data %>%
  distinct(unique_id) %>%
  nrow()

unique_infants <- ma_data %>% 
  filter(unique_infant == "unique_condition") %>% 
  filter(adult_participant == "no") %>% 
  summarize(sum_infants = sum(n_1))

non_unique_infants <- ma_data %>% 
  filter(unique_infant == "not_unique") %>% 
  filter(adult_participant == "no") %>%
  distinct(short_cite, expt_condition, .keep_all = TRUE) %>% 
  summarize(sum_infants = sum(n_1))

different_infants <- unique_infants + non_unique_infants
mean_age_day <- ma_data %>% filter(!is.na(mean_age)) %>% summarize(mean_age =  format(round(mean(mean_age), 2)))

```
  
<!-- There's a bunch of missing values in the pdf, not sure why--> 
   Our final sample included data from `r different_infants` unique infants (Mean age: `r mean_age_day` Days), reported in `r n_effect_sizes` individual effect sizes from `r n_papers` individual papers. 
   
## Data Entry

For each paper, we entered the paper metadata (e.g., citation), information to calculate effect sizes, and moderators. We entered a separate effect size for each experimental condition and age group in paper. Most papers therefore contained multiple effect sizes.

```{r data_source_summary}
data_source_summary <- ma_data %>% group_by(data_source_clean) %>% count()
num_author_contact <- data_source_summary %>% filter(data_source_clean == "author_contact") %>% pull(n)
num_plot <- data_source_summary %>% filter(data_source_clean == "plot") %>% pull(n)
num_table <- data_source_summary %>% filter(data_source_clean == "table") %>% pull(n)
num_text <- data_source_summary %>% filter(data_source_clean == "text") %>% pull(n)

```

In order to calculate the effect sizes, we recorded the sample size of each condition, the group mean, and the across-participant standard deviation. The mean and standard deviation were obtained from one of the three ways: a) directly retrieved from the results section or the data-presenting tables (N = `r num_table + num_text`); b) recovered from the plots by measuring the height of the bars and the error bars (N =`r num_plot`); c) contacted the original authors (N = `r num_author_contact`). For looking time studies, when the paper only reported the raw looking time in seconds, we calculated the proportion of correct response by dividing the mean looking time toward the matching scenes by the sum of mean looking time toward the matching scenes and the mean looking time toward the non-matching scenes (i.e., excluding the look away time from the denominator). The standard deviations were also scaled by being divided by the sum. <!-- what do you mean by scaled here?-->


```{r categorize_by_proto}
num_proto_approach <- ma_data %>% filter (inclusion_certainty == 2) %>% count()
num_atypical_approach <- ma_data %>% filter (inclusion_certainty == 1) %>% count()
```

We classified our final samples of effect sizes (N = `r n_effect_sizes`) into two categories: the prototypical ones (N = `r num_proto_approach`) and atypical ones (N = `r num_atypical_approach`). Prototypical studies were the ones in which the set-up and the procedures were most similar to the seminal paper [@naigles1990children]. The atypical studies were experiments that met all of the inclusion criteria, but deviated from the prototypical designs in some non-trivial ways (The stimuli involved more complex syntactic structure or event structure: N = `r n_effect_sizes - 12`; The testing procedure included a contrast phase: N = 10; The experiment was conducted through online platform: N = 2). To preserve the homogeneity of the experiments without reducing the power of our analysis, we conducted our statistical analysis both on the entire sample and specifically on the prototypical studies.

```{r vocb_available}
num_vocabulary_available <- ma_data %>% filter(!is.na(productive_vocab_median)) %>% count()
```

For all experimental conditions, we coded three types of moderators: participant moderators,  stimuli moderators, and  procedure moderators. The information was either retrieved from the methods section of the paper or obtained via contacting authors. 

At the participant level, we coded infants' mean age in days (N= `r n_effect_sizes`) and the median productive vocabulary measured by MacArthur-Bates Communicative Development Inventories (CDI) Words and Sentences <!--add citation--> (`r  num_vocabulary_available`). 

```{r categorize_by_linguistic_stimuli}
num_transitive <- ma_data %>% filter(sentence_structure == "transitive") %>% count()
num_intransitive <- ma_data %>% filter(sentence_structure == "intransitive") %>% count()

num_agent_sum <- ma_data %>% count(agent_argument_type_clean)
num_agent_noun <- num_agent_sum %>% filter(agent_argument_type_clean == "noun") %>% pull(n)
num_agent_noun_phrase <- num_agent_sum %>% filter(agent_argument_type_clean == "noun_phrase") %>% pull(n) 
num_agent_pronoun <- num_agent_sum %>% filter(agent_argument_type_clean == "noun_phrase") %>% pull(n)
num_agent_two_noun <- num_agent_sum %>% filter(agent_argument_type_clean == "two_noun") %>% pull(n)
num_agent_varying <- num_agent_sum %>% filter(agent_argument_type_clean == "varying_agent") %>% pull(n)

num_patient_sum <-  ma_data %>% count(patient_argument_type_clean)
num_patient_noun <- num_patient_sum %>% filter(patient_argument_type_clean == "noun") %>% pull(n)
num_patient_noun_phrase <- num_patient_sum %>% filter(patient_argument_type_clean == "noun_phrase") %>% pull(n)
num_patient_pronoun <- num_patient_sum %>% filter(patient_argument_type_clean == "pronoun") %>% pull(n)
num_patient_varying <- num_patient_sum %>% filter(patient_argument_type_clean == "varying_patient") %>% pull(n)

num_test_agent <- ma_data %>% filter(test_type == "agent") %>% count()
num_test_action <- ma_data %>% filter(test_type == "action") %>% count()

```

We coded features of the linguistic stimuli during both training and testing. The training linguistic stimuli were categorized as transitive (N = `r num_intransitive`), if the novel verbs were embedded in a sentence with two or more noun arguments, and  intransitive (N = `r num_intransitive`) if the novel verbs were embedded in a sentence with one noun argument. We also coded the types of words used in the agent argument (One noun: N = `r num_agent_noun`; One pronoun: N = `r num_agent_pronoun`; Two nouns: N = `r num_agent_two_noun`; Noun phrase: N = `r num_agent_noun_phrase`; Varying across sentences: N = `r num_agent_varying`) and the patient argument(One noun: N = `r num_patient_noun`; One pronoun: N = `r num_patient_pronoun`; Noun phrase: N = `r num_patient_noun_phrase`; Varying across sentences: N = `r num_patient_varying`). We coded the  testing linguistic stimuli for whether infants were prompted to identify the action (e.g. "Where's lorping? Find lorping!", N = `r num_test_agent`) or the actors (e.g. "Which one (verbed) the other one....point!", N = `r num_test_action`). 

```{r categorize_by_visual_stimuli}
num_modality_sum <- ma_data %>% count(stimuli_modality)
num_modality_animation <- num_modality_sum %>% filter(stimuli_modality == "animation") %>% pull(n)
num_modality_video <- num_modality_sum %>%  filter(stimuli_modality == "video") %>%  pull(n)

num_video_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "person") %>% count()
num_video_non_person <- ma_data %>% filter(stimuli_modality == "video") %>% filter(stimuli_actor == "non_person") %>% count()

num_video_simultaneous <- ma_data %>% filter(presentation_type == "simultaneous") %>% count()
num_video_immediate_after <- ma_data %>% filter(presentation_type == "immediate_after") %>% count()
num_video_asynchronous <- ma_data %>% filter(presentation_type == "asynchronous") %>% count()


```

For visual stimuli, we coded the types of media used (video or animation) and the types of protagonists in the events (person or non-person). `r num_modality_video` conditions used video recordings of human actors (N = `r num_video_person`) or human actors in animal suits (N = `r num_video_non_person`). The other `r num_modality_animation` conditions used clips of animation with non-person figures as the protagonists of the events. We also coded how the onset of the linguistic stimuli aligned with the visual stimuli. The procedure was coded as "simultaneous" if the very first training sentence was presented along with the visual stimuli depicting relevant action (N = `r num_video_simultaneous`). It was coded as "immediately after" if it was presented along with an attention-getter or a blank screen, immediately followed by the relevant action (N = `r num_video_immediate_after`). Finally, some experimental conditions first presented the linguistic stimuli paired with irrelevant visual scenes (e.g. a person on the phone talking). The relevant visual stimuli were not shown until the training phase is over. For experimental conditions using this alignment, they were coded as "asynchronous"(N = `r num_video_asynchronous`).

```{r categorize_by_procedure}
num_point <- ma_data %>% filter(test_method == "point") %>% count()
num_look <- ma_data %>% filter(test_method == "look") %>% count()

num_practice_yes <- ma_data %>% filter(practice_phase == "yes") %>% count()
num_practice_no <- ma_data %>% filter(practice_phase == "no") %>% count()
num_char_id_yes <- ma_data %>% filter(character_identification == "yes") %>% count()
num_char_id_no <- ma_data %>% filter(character_identification == "no") %>% count()

num_mass <- ma_data %>% filter(test_mass_or_distributed == "mass") %>% count()
num_distributed <- ma_data %>% filter(test_mass_or_distributed == "distributed") %>% count()
```

Finally, procedure moderators included the types of response elicited from the participants: whether the infants were explicitly prompted to point or their eye gaze duration were measured as they heard the linguistic stimuli (Pointing: N = `r num_point`; Looking: N = `r num_look`). Three characteristics of the experimental procedures were coded as categorical variables: the inclusion of practice phase (Yes: N = `r num_practice_yes` ; No: N = `r num_practice_no`), the inclusion of character-identification phase (Yes: N = `r num_char_id_yes`; No: N = `r num_char_id_no`), and the distribution of the training and the testing trials (Mass: N = `r num_mass`; Distributed: N = `r num_distributed`). A procedure was categorized as "mass" if and only if the infants were trained exclusively on one novel verb and tested on the very same verb. It was "distributed" if the infants were given multiple train and test pairs on multiple novel verbs. To better characterize the experience infants had prior to testing, we also coded how many train-test pair the infants were given (for mass procedure it was always 1), how many trials during the test phase infants were given, how many times the visual stimuli showing the relevant actions were presented, and how many times each novel verb was spoken in a syntactically-informative way. 
  
## Data Preprocessing 

```{r data_preprocessing}
num_has_raw <- ma_data %>% filter(!is.na(x_2_raw)) %>% count()
```

  We calculated the Cohen's d effect size for each experimental condition. Because we only included studies that used a two alternative forced-choice test method, we compared the mean proportion of correct response against chance level (0.5) for the group. Although some papers collected a baseline measure (N = `r num_has_raw`; e.g. XXX, XXX), most papers did not. In order to calculate a consistent effect size measure across all condition, we used chance level as a baseline for calculating effect size for all conditions (see SI for comparision of effect sizes using reported baseline versus chance). 
  
  All the effect sizes and the coded variables were then analyzed with the metafor package in R [@viechtbauer2010].

# Results

```{r child = "sections/section1.Rmd"}
```

# General Discussion


\newpage

# References

---
nocite: |
...
  
```{r appendix}
render_appendix("appendix.Rmd")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
